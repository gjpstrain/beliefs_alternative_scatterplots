---
format: acm-pdf

# use keep-tex to cause quarto to generate a .tex file
# which you can eventually use with TAPS

keep-tex: true

params:
  eval_models: true
  
knitr:
  opts_chunk: 
    cache_comments: false
    crop: true
    
execute: 
  echo: false
  warning: false
  message: false
  include: false

bibliography: atypical-scatterplots.bib

title: Effects of Alternative Scatterplots Designs on Belief Change

# if short-title is defined, then it's used
short-title: 

author:
  - name: Gabriel Strain
    email: gabriel.strain@manchester.ac.uk
    orcid: 0000-0002-4769-9221
    affiliation:
      name: Department of Computer Science, Faculty of Science and Engineering, University of Manchester
      address: Oxford Road
      city: Manchester
      country: United Kingdom
      postal-code: M13 9PL
  - name: Andrew J. Stewart
    email: andrew.j.stewart@manchester.ac.uk
    affiliation:
      name: Department of Computer Science, Faculty of Science and Engineering, University of Manchester
      address: Oxford Road
      city: Manchester
      country: United Kingdom
      postal-code: M13 9PL
  - name: Paul Warren
    email: paul.warren@manchester.ac.uk
    affiliation:
      name: Division of Psychology, Communication and Human Neuroscience, School of Health Sciences, Faculty of Biology, Medicine, and Health, University of Manchester
      address: Oxford Road
      city: Manchester
      country: United Kingdom
      postal-code: M13 9PL
  - name: Charlotte Rutherford
    email: charlotte.rutherford-2@postgrad.manchester.ac.uk
    affiliation:
      name: Division of Psychology Communication and Human Neuroscience, School of Health Sciences, Faculty of Biology, Medicine, and Health, University of Manchester
      address: Oxford Road
      city: Manchester
      country: United Kingdom
      postal-code: M13 9PL
  - name: Caroline Jay
    affiliation:
      name: Department of Computer Science, Faculty of Science and Engineering, University of Manchester
      address: Oxford Road
      city: Manchester
      country: United Kingdom
      postal-code: M13 9PL

# acm-specific metadata
acm-metadata:
  # comment this out to make submission anonymous
  anonymous: true

  # comment this out to build a draft version
  # final: false

  # comment this out to specify detailed document options
  # acmart-options: sigconf, review, screen  

  # acm preamble information
  copyright-year: 2018
  acm-year: 2018
  copyright: acmcopyright
  doi: XXXXXXX.XXXXXXX
  conference-acronym: "Conference acronym 'XX"
  conference-name: |
    Make sure to enter the correct
    conference title from your rights confirmation emai
  conference-date: June 03--05, 2018
  conference-location: Woodstock, NY
  price: "15.00"
  isbn: 978-1-4503-XXXX-X/18/06

  # if present, replaces the list of authors in the page header.
  shortauthors: Strain et al.

  # The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
  # Please copy and paste the code instead of the example below.
  ccs: |
    \begin{CCSXML}
    <ccs2012>
     <concept>
      <concept_id>10010520.10010553.10010562</concept_id>
      <concept_desc>Computer systems organization~Embedded systems</concept_desc>
      <concept_significance>500</concept_significance>
     </concept>
     <concept>
      <concept_id>10010520.10010575.10010755</concept_id>
      <concept_desc>Computer systems organization~Redundancy</concept_desc>
      <concept_significance>300</concept_significance>
     </concept>
     <concept>
      <concept_id>10010520.10010553.10010554</concept_id>
      <concept_desc>Computer systems organization~Robotics</concept_desc>
      <concept_significance>100</concept_significance>
     </concept>
     <concept>
      <concept_id>10003033.10003083.10003095</concept_id>
      <concept_desc>Networks~Network reliability</concept_desc>
      <concept_significance>100</concept_significance>
     </concept>
    </ccs2012>
    \end{CCSXML}
    
    \ccsdesc[500]{Computer systems organization~Embedded systems}
    \ccsdesc[300]{Computer systems organization~Redundancy}
    \ccsdesc{Computer systems organization~Robotics}
    \ccsdesc[100]{Networks~Network reliability}

  keywords:
    - belief change
    - correlation perception
    - scatterplot
    - crowdsourced
  
abstract: |
  Viewers tend to underestimate correlation in positively correlated scatterplots. However, systematically changing the size and opacity of scatterplot points can bias estimates upwards, correcting this underestimation in a simple estimation paradigm. Here we examine whether application of these visualization techniques goes beyond a simple perceptual effect and could actually influence beliefs about information from trusted news sources. We present a fully-reproducible study in which we demonstrate that scatterplot manipulations that can correct for the correlation underestimation bias can also induce stronger levels of belief change compared to unadapted scatterplots presenting identical data. Consequently, we show that novel visualization techniques can be used to drive belief change, and suggest future directions for extending this work with regards to altering attitudes and behaviours.
---

```{r}
#| label: setup

set.seed(1234) # random seed for number generation

library(tidyverse)
library(MASS)
library(emmeans)
library(scales)
library(buildmer)
library(lme4)
library(kableExtra)
library(papaja)
library(qwraps2)
library(lmerTest)
library(ggdist)
library(ggpubr)
library(conflicted)
library(ggtext)
library(Matrix)
library(irr)
library(bbplot)
library(geomtextpath)
library(ggh4x)
library(ordinal)

# fix conflicts now using the conflicted package

conflicts_prefer(dplyr::select(), dplyr::filter(), lme4::lmer())

options(knitr.kable.NA = "")

```

```{r}
#| label: lazyload-cache

if (!params$eval_models){ lazyload_cache_dir("beliefs_attitudes_atypical_scatterplots_cache/pdf") }
```

```{r}
#| label: load-data

# load in data csvs for pre-test and main experiment, remove the "__participant" column now 

pre_test_anon <- read_csv("data/pre_test_response_final.csv") %>% select(-"__participant")

main_exp_A <- read_csv("data/main_test_A_final.csv") %>% select(-"__participant")

main_exp_T <- read_csv("data/main_test_T_final.csv") %>% select(-"__participant")
```

```{r}
#| label: wrangle-data

## NB: With the exception of anonymization, data are provided as-is from 
## pavlovia (survey tool). Wrangling functions *must* be run first to make
## the data sets usable

wrangle_pre_test <- function(anon_file) {
  
# extract demographic information
  
demographics <- anon_file %>%
  filter(!is.na(gender_slider.response)) %>%
  select(matches(c("participant",
                   "age_textbox.text",
                   "gender_slider.response")))

time_taken <- anon_file %>%
  filter(!is.na(final.stopped)) %>%
  select(c("participant", "final.stopped"))
  

# select relevant columns
# select only experimental items
# change data types where appropriate
# output this file with suffix 'tidy'

anon_file %>%
  select(c("participant",
           "avg_corr",
           "avg_emot",
           "slider_emotion.response",
           "slider_belief.response",
           "statement",
           "item_no",
           "label",
           "session",
           )) %>%
  filter(item_no < 26) %>%
  full_join(demographics, by = "participant") %>%
  full_join(time_taken, by = "participant") %>%
  assign(paste0(unique(anon_file$expName), "_tidy"),
           value = ., envir = .GlobalEnv)
}

wrangle_main_exp <- function(anon_file) {
  
# extract demographic information
  
demographics <- anon_file %>%
  filter(!is.na(gender_slider.response)) %>%
  select(matches(c("participant",
                   "age_textbox.text",
                   "gender_slider.response")))

# extract time taken information

time_taken <- anon_file %>%
  filter(!is.na(final.stopped)) %>%
  select(c("participant", "final.stopped"))

# extract defensive confidence testing scores

defensive_confidence <- anon_file %>%
  select(contains(c("DC", "participant"))) %>% 
  select(-contains(c("stopped", "started"))) %>%
  rename(DC_1 = q1_slider_DC.response,
         DC_2 = q2_slider_DC.response,
         DC_3 = q3_slider_DC.response,
         DC_4 = q4_slider_DC.response,
         DC_5 = q1_slider_DC_2.response,
         DC_6 = q2_slider_DC_2.response,
         DC_7 = q3_slider_DC_2.response,
         DC_8 = q4_slider_DC_2.response,
         DC_9 = q1_slider_DC_3.response,
         DC_10 = q2_slider_DC_3.response,
         DC_11 = q3_slider_DC_3.response,
         DC_12 = q4_slider_DC_3.response) %>%
  mutate(DC_3 = 6 - DC_3,                # reverse score items 3, 4, 10, 12
         DC_4 = 6 - DC_4,
         DC_10 = 6 - DC_10,
         DC_12 = 6 - DC_12) %>%
  group_by(participant) %>%
  summarise(
    dc_score = sum(DC_1,
                   DC_2,
                   DC_3,
                   DC_4,
                   DC_5,
                   DC_6,
                   DC_7,
                   DC_8,
                   DC_9,
                   DC_10,
                   DC_11,
                   DC_12,
                   na.rm = T)
  )

# extract literacy info

literacy <- anon_file %>%
    filter(!is.na(q5_slider.response)) %>%
    rowwise() %>%
    mutate(literacy = sum(c(q1_slider.response, 
                            q2_slider.response, 
                            q3_slider.response, 
                            q4_slider.response, 
                            q5_slider.response))) %>%
    select(participant,
           literacy)

# split unique item no column into number (dataset used) and letter (condition)

anon_file <- anon_file %>%
  separate(starts_with("unique"), into = c("item_no", "condition"), sep = "(?<=\\d)(?=\\D)")

# extract pre belief, post belief, and belief_diff

belief <- anon_file %>%
  group_by(participant) %>%
  summarise(
    pre_bel = sum(slider_belief.response, na.rm = T),
    post_bel = sum(slider_belief_post.response, na.rm = T)
  ) %>%
  mutate(belief_diff = post_bel - pre_bel) %>%
  select(participant, belief_diff, pre_bel, post_bel)

emotion <- anon_file %>%
  filter(!is.na(slider_emotion.response)) %>%
  group_by(participant) %>%
  select(participant, slider_emotion.response)

# select relevant columns
# select only experimental items
# change data types where appropriate
# output this file with suffix 'tidy'

anon_file %>%
  select(c("participant",
           "item_no",
           "condition",
           "slider.response",
           "trials.thisN"
           )) %>%
  mutate(half = case_when(
    trials.thisN < 23 ~ "first",
    trials.thisN > 23 ~ "second")) %>%
  filter(item_no < 46) %>%
  inner_join(literacy, by = "participant") %>%
  inner_join(demographics, by = "participant") %>%
  inner_join(belief, by = "participant") %>%
  inner_join(emotion, by = "participant") %>%
  inner_join(defensive_confidence, by = "participant") %>%
  full_join(time_taken, by = "participant") %>%
  mutate(across(matches(c("item_no", "condition")), as_factor)) %>%
  mutate(trials.thisN = as.integer(trials.thisN)) %>%
  pivot_longer(cols = c("pre_bel", "post_bel"),
               values_to = "rating",
               names_to = "rating_time") %>%
  mutate(rating_time = as_factor(rating_time)) %>%
  mutate(rating_time = fct_relevel(rating_time, "post_bel", "pre_bel")) %>% # reverse code factor levels so that clmm
  mutate(across(c("rating"), as.ordered)) %>%                               # interprets before viewing as reference level
  assign(paste0(unique(anon_file$expName), "_tidy"),
           value = ., envir = .GlobalEnv)

}

# use wrangling function on anonymized data files

wrangle_pre_test(pre_test_anon)

wrangle_main_exp(main_exp_A)

wrangle_main_exp(main_exp_T)

# add 75 to each participant number for second part of main exp

atypical_scatterplots_main_test_T_tidy <- atypical_scatterplots_main_test_T_tidy %>% mutate(participant = participant + 75)

# main experiment is between participants, so rbind main_exp dfs together

main_exp_tidy <- rbind(atypical_scatterplots_main_test_A_tidy,
                       atypical_scatterplots_main_test_T_tidy)

# set deviation coding for experimental model

contrasts(main_exp_tidy$condition) <- matrix(c(.5, -.5))


main_exp_tidy <- main_exp_tidy %>%              # ended up not using this, but
  mutate(dc_section = case_when(                # leaving it in in case it's
    dc_score < 20 ~ "low",                      # useful to you 
    dc_score > 20 & dc_score < 40 ~ "average",
    dc_score > 40 ~ "high"
  ))

# check for missing age and gender id values

sum(is.na(beliefs_scatterplots_pretest_tidy$gender_slider.response))

sum(is.na(beliefs_scatterplots_pretest_tidy$age_textbox.text))

# missing age and gender values for a single participant
# print session with missing values

beliefs_scatterplots_pretest_tidy %>%
  filter(is.na(age_textbox.text)) %>%
  distinct(session)

# session id was used to look up age and gender identity and manually add
# this information to the final results csv using the prolific-supplied
# demographic information for the pretest
# this is not included as would expose personally identifiable prolific IDs

# Extract gender data into separate dfS

gender_pre <- distinct(beliefs_scatterplots_pretest_tidy, participant,
                      .keep_all = TRUE) %>%
  group_by(gender_slider.response) %>%
  summarise(perc = n()/nrow(.)*100) %>%
  pivot_wider(names_from = gender_slider.response, values_from = perc)

gender_main <- distinct(main_exp_tidy, participant,
                      .keep_all = TRUE) %>%
  group_by(gender_slider.response) %>%
  summarise(perc = n()/nrow(.)*100) %>%
  pivot_wider(names_from = gender_slider.response, values_from = perc)

# extract age data into separate dfS

age_pre <- distinct(beliefs_scatterplots_pretest_tidy, participant,
                .keep_all = TRUE) %>%
  summarise(mean = mean(age_textbox.text, na.rm = TRUE),
            sd = sd(age_textbox.text, na.rm = TRUE))

age_main <- distinct(main_exp_tidy, participant,
                .keep_all = TRUE) %>%
  summarise(mean = mean(age_textbox.text, na.rm = TRUE),
            sd = sd(age_textbox.text, na.rm = TRUE)) 

# extract time taken data into separate dfs

time_taken_pre <- distinct(beliefs_scatterplots_pretest_tidy, participant,
                .keep_all = TRUE) %>%
  summarise(mean = mean(final.stopped/60, na.rm = TRUE),
            sd = sd(final.stopped/60, na.rm = TRUE)) 

time_taken_main <- distinct(main_exp_tidy, participant,
                .keep_all = TRUE) %>%
  summarise(mean = mean(final.stopped/60, na.rm = TRUE),
            sd = sd(final.stopped/60, na.rm = TRUE))

# extract literacy data into separate df

literacy <- distinct(main_exp_tidy, participant,
                        .keep_all = TRUE) %>%
  summarise(mean = mean(literacy), sd = sd(literacy))

# extract defensive confidence data into separate df

def_con <- distinct(main_exp_tidy, participant,
                        .keep_all = TRUE) %>%
  summarise(mean = mean(dc_score), sd = sd(dc_score))

# extract topic emotionality data into separate df

topic_emo <- distinct(main_exp_tidy, participant,
                        .keep_all = TRUE) %>%
  summarise(mean = mean(slider_emotion.response), sd = sd(slider_emotion.response))

# set deviation coding contrasts

contrasts(main_exp_tidy$rating_time) <- matrix(c(.5, -.5))
contrasts(main_exp_tidy$condition) <- matrix(c(.5, -.5))

# remove all intermediate dfs

rm(main_exp_A,
   main_exp_T,
   atypical_scatterplots_main_test_A_tidy,
   atypical_scatterplots_main_test_T_tidy,
   pre_test_anon)
```

```{r}
#| label: contrasts-extract

# this function extracts test statistics and p values from model summaries

contrasts_extract <- function(model) {
  
  model_name <- deparse(substitute(model))
  
  if (class(model) == "buildmer") model <- model@model
  
  EMMs <- emmeans(model, pairwise ~ rating_time * condition)
  
  contrast_df <- as.data.frame(EMMs[2]) %>%
                            rename_with(str_replace,
                                        pattern = "contrasts.", replacement = "",
                                        matches("contrasts")) %>%
                            rename_with(str_to_title, !starts_with("p")) %>%
                            select(c("Contrast", "Z.ratio", "p.value"))
  
  return(contrast_df)
}
```

```{r}
#| label: plot-examples-function
  
  set.seed(1234)
  
  my_sample_size = 128
  
  my_desired_r = 0.6
  
  mean_variable_1 = 0
  sd_variable_1 = 1
  
  mean_variable_2 = 0
  sd_variable_2 = 1
  
  mu <- c(mean_variable_1, mean_variable_2) 
  
  myr <- my_desired_r * sqrt(sd_variable_1) * sqrt(sd_variable_2)
  
  mysigma <- matrix(c(sd_variable_1, myr, myr, sd_variable_2), 2, 2) 
  
  corr_data = as_tibble(mvrnorm(my_sample_size, mu, mysigma, empirical = TRUE))
  
  corr_model <- lm(V2 ~ V1, data = corr_data)
  
  my_residuals <- abs(residuals(corr_model))
  
  data_with_resid <- round(cbind(corr_data, my_residuals), 2)
  
slopes <- data_with_resid %>%
  mutate(slope_0.25 = 1-(0.25)^my_residuals) %>%
  mutate(slope_inverted = (1 + (0.25)^ my_residuals)-1) %>%
  mutate(slope_inverted_floored = pmax(0.1,(1+(0.25)^my_residuals)-1)) 
  
plot_example_function <- function (data, size, opacity, title) {
  
set.seed(1234)
  
  ggplot(data, aes(x = V1, y = V2)) +
  scale_size_identity() +
  scale_alpha_identity() +
  geom_point(aes(size = 2*(size + 0.1),
                 alpha = opacity),
             shape = 16)  +
  theme_ggdist() +
  theme(axis.text = element_blank(),
        plot.margin = unit(c(0,0,0,0), "cm"),
        legend.position = "none",
        plot.title = element_text(size = 8.5, vjust = -0.1),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        axis.line = element_line(linewidth = 0.25),
        axis.ticks = element_blank()) +
  labs(title = title)

} 
```

```{r}
#| label: format-numbers-function

# function to format numbers when papaja prints them

add_and_to_numbers <- function(numbers) {

  num_str <- as.character(numbers)

  len <- length(num_str)
  
  if (len == 1) {

    return(num_str)
    
  } else if (len == 2) {
    
    return(paste(num_str[1], "and", num_str[2]))
    
  } else {

    return(paste(paste(num_str[1:(len-1)], collapse = ", "), ", and ", num_str[len], sep = ""))
  }
}

```

```{r}
#| label: author-pre-test-ratings

# NB: this code is reproduced here so that test values are accessible
# It can also be found at item_preparation/pre_test_stim.R

# Create dataframes for each rater

author_A_statements <- read_csv("item_preparation/statements_A.csv") %>%
  rename(Topic_Emotionality_A = Topic_Emotionality,
         Strength_of_Correlation_A = Strength_of_Correlation) %>%
  select(-Notes)
  

author_B_statements <- read_csv("item_preparation/statements_B.csv") %>%
  rename(Topic_Emotionality_B = Topic_Emotionality,
         Strength_of_Correlation_B = Strength_of_Correlation) %>%
  select(-Notes)

# Bind dataframes together

all_ratings <- left_join(author_A_statements,
                         author_B_statements,
                         by = c("Number",
                                "Statements"))

## IRR Calculations

irr_emot <- kappa2(matrix(c(all_ratings$Topic_Emotionality_A,
                all_ratings$Topic_Emotionality_B), ncol = 2), "squared")    

irr_corr <- kappa2(matrix(c(all_ratings$Strength_of_Correlation_A,
                all_ratings$Strength_of_Correlation_B), ncol = 2), "squared")    

```

```{r}
#| label: comparison-function

# this function takes a model and creates a nested model with the fixed effects 
# terms removed for anova comparison

comparison <- function(model) {
  
  parens <- function(x) paste0("(",x,")")
  onlyBars <- function(form) reformulate(sapply(findbars(form),
                                              function(x)  parens(deparse(x))),
                                       response=".")
  onlyBars(formula(model))
  cmpr_model <- update(model,onlyBars(formula(model)))
  
  return(cmpr_model)
  
}
```

```{r}
#| label: anova-results-functions

# this function takes two nested models, runs an anova, and the outputs the 
# test statistic, the degrees of freedom, and the p value to the global 
# environment

anova_results_mixed <- function(model, cmpr_model) {
  
  model_name <- deparse(substitute(model))
  
  if (class(model) == "buildmer") model <- model@model
  if (class(cmpr_model) == "buildmer") cmpr_model <- cmpr_model@model
  
  if (class(model) == "clmm") {
  
  anova_output <- ordinal:::anova.clm(model, cmpr_model)
  
  assign(paste0(model_name, ".LR"),
         anova_output$LR.stat[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".df"),
         anova_output$df[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".p"),
         anova_output$`Pr(>Chisq)`[2],
         envir = .GlobalEnv)
  }
  
  else
    
  anova_output <- anova(model, cmpr_model)
  
  assign(paste0(model_name, ".Chisq"),
         anova_output$Chisq[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".df"),
         anova_output$df[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".p"),
         anova_output$`Pr(>Chisq)`[2],
         envir = .GlobalEnv)
}

anova_results_linear <- function(cmpr_model, model) { # currently unused, left it for the craic
  
  model_name <- deparse(substitute(model))
  
  if (class(model) == "buildmer") model <- model@model
  if (class(cmpr_model) == "buildmer") cmpr_model <- cmpr_model@model
  
  anova_output <- anova(cmpr_model, model)
  
  assign(paste0(model_name, ".F"),
         anova_output$'F'[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".df"),
         anova_output$Df[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".p"),
         anova_output$`Pr(>F)`[2],
         envir = .GlobalEnv)
  
} # currently unused
```

```{r}
#| label: make-sig-tables

# functions to create tables with fixed effects and interactions for models

# first do function for absolute differences model (H1)

make_sig_table_abso <- function (model) {
  
  # buildmer class models need reassigned to lmer class for further use
  
  if (class(model) == "buildmer") model <- model@model
  
  # subset model summary to get list of fixed/interaction effects
  # then make this a data frame, rename columns, fix p value formatting
  
  model_summary <- summary(model)
  
  coef_table <- model_summary$coefficients %>% tail(1)

  df <- as_tibble(coef_table, rownames = "Effect") %>%
    rename("Standard Error" = "Std. Error",
           "Z-value" = "z value",
    "p" = "Pr(>|z|)") %>%
    mutate(p = scales::pvalue(p)) %>%
    rename("\\textit{p}" = "p") %>%
    mutate("Odds Ratio" = exp(Estimate)) %>%
    mutate("Cohen's \\textit{d}" = effectsize::oddsratio_to_d(`Odds Ratio`)) %>%
    mutate("Effect" = recode(Effect,
                               "rating_time1" = "Rating Time")) %>%
    column_to_rownames(var = "Effect")

  table <- kbl(df, booktabs = T, digits = c(2,3,2,2,2,2,3), escape = F)
  
  return(table)
}

# currently unused functions for sig tables re additional analyses

make_sig_table_rel <- function (model) {
  
  # buildmer class models need reassigned to lmer class for further use
  
  if (class(model) == "buildmer") model <- model@model
  
  # subset model summary to get list of fixed/interaction effects
  # then make this a data frame, rename columns, fix p value formatting
  
  model_summary <- summary(model)
  
  coef_table <- model_summary$coefficients %>% tail(3)


  df <- as_tibble(coef_table, rownames = "Effect") %>%
    rename("Standard Error" = "Std. Error",
           "Z-value" = "z value",
    "p" = "Pr(>|z|)") %>%
    mutate(p = scales::pvalue(p)) %>%
    rename("\\textit{p}" = "p") %>%
    mutate("Odds Ratio" = exp(abs(Estimate))) %>%
    mutate("Cohen's \\textit{d}" = effectsize::oddsratio_to_d(`Odds Ratio`)) %>%
    mutate("Effect" = recode(Effect,
                               "rating_time1" = "Rating Time",
                               "condition1" = "Condition",
                               "rating_time1:condition1" = "Rating Time x Condition")) %>%
    column_to_rownames(var = "Effect")
 
  table <- kbl(df, booktabs = T, digits = c(2,3,2,2,2,2,3), escape = F)
  
  return(table)
}

make_sig_table_lit <- function (model) {
  
  # buildmer class models need reassigned to lmer class for further use
  
  if (class(model) == "buildmer") model <- model@model
  
  # subset model summary to get list of fixed/interaction effects
  # then make this a data frame, rename columns, fix p value formatting
  
  model_summary <- summary(model)
  
  coef_table <- model_summary$coefficients

  df_residual <- model_summary$df[2]
  df_model <- model_summary$df[1]
  
  coef_table <- cbind(coef_table, "df" = c(rep(df_model, nrow(coef_table) - 1), df_residual))

  df <- as_tibble(coef_table, rownames = "Effect") %>%
    rename("Standard Error" = "Std. Error",
           "t-value" = "t value",
    "p" = "Pr(>|t|)") %>%
    mutate(p = scales::pvalue(p)) %>%
    rename("\\textit{p}" = "p") %>%
    relocate(df, .after = "Standard Error") 
  
    r_squared <- r2beta(lit_model, method = "nsj") %>%
    select(c("Effect", "Rsq")) %>%
    mutate(Rsq = round(Rsq, 3)) %>%
    mutate(Rsq = ifelse(row_number() == 1, "", Rsq)) %>%
    rename("R\\textsuperscript{2}" = "Rsq") 
    
    df <- left_join(df, r_squared, by = "Effect") %>%
      mutate("Effect" = recode(Effect,
                               "condition1" = "Condition",
                               "literacy" = "Literacy",
                               "condition1:literacy" = "Condition * Literacy")) %>%
      column_to_rownames(var = "Effect")
  
  table <- kbl(df, booktabs = T, digits = c(2,3,2,2,2,2,3), escape = F)
  
  return(table)
}

make_sig_table_dc <- function (model) {
  
  # buildmer class models need reassigned to lmer class for further use
  
  if (class(model) == "buildmer") model <- model@model
  
  # subset model summary to get list of fixed/interaction effects
  # then make this a data frame, rename columns, fix p value formatting
  
  model_summary <- summary(model)
  
  coef_table <- model_summary$coefficients

  df_residual <- model_summary$df[2]
  df_model <- model_summary$df[1]
  
  coef_table <- cbind(coef_table, "df" = c(rep(df_model, nrow(coef_table) - 1), df_residual))

  df <- as_tibble(coef_table, rownames = "Effect") %>%
    rename("Standard Error" = "Std. Error",
           "t-value" = "t value",
    "p" = "Pr(>|t|)") %>%
    mutate(p = scales::pvalue(p)) %>%
    rename("\\textit{p}" = "p") %>%
    relocate(df, .after = "Standard Error") 
  
    r_squared <- r2beta(model, method = "nsj") %>%
    select(c("Effect", "Rsq")) %>%
    mutate(Rsq = round(Rsq, 3)) %>%
    mutate(Rsq = ifelse(row_number() == 1, "", Rsq)) %>%
    rename("R\\textsuperscript{2}" = "Rsq") 
    
    df <- left_join(df, r_squared, by = "Effect") %>%
      mutate("Effect" = recode(Effect,
                               "condition1" = "Condition",
                               "dc_score" = "Defensive Confidence",
                               "condition1:dc_score" = "Condition * Defensive Confidence")) %>%
      column_to_rownames(var = "Effect")
  
  table <- kbl(df, booktabs = T, digits = c(2,3,2,2,2,2,3), escape = F)
  
  return(table)
}

make_sig_table_emo <- function (model) {
  
  # buildmer class models need reassigned to lmer class for further use
  
  if (class(model) == "buildmer") model <- model@model
  
  # subset model summary to get list of fixed/interaction effects
  # then make this a data frame, rename columns, fix p value formatting
  
  model_summary <- summary(model)
  
  coef_table <- model_summary$coefficients

  df_residual <- model_summary$df[2]
  df_model <- model_summary$df[1]
  
  coef_table <- cbind(coef_table, "df" = c(rep(df_model, nrow(coef_table) - 1), df_residual))

  df <- as_tibble(coef_table, rownames = "Effect") %>%
    rename("Standard Error" = "Std. Error",
           "t-value" = "t value",
    "p" = "Pr(>|t|)") %>%
    mutate(p = scales::pvalue(p)) %>%
    rename("\\textit{p}" = "p") %>%
    relocate(df, .after = "Standard Error") 
  
    r_squared <- r2beta(model, method = "nsj") %>%
    select(c("Effect", "Rsq")) %>%
    mutate(Rsq = round(Rsq, 3)) %>%
    mutate(Rsq = ifelse(row_number() == 1, "", Rsq)) %>%
    rename("R\\textsuperscript{2}" = "Rsq") 
    
    df <- left_join(df, r_squared, by = "Effect") %>%
      mutate("Effect" = recode(Effect,
                               "condition1" = "Condition",
                               "slider_emotion.response" = "Statement Emotionality Rating",
                               "condition1:slider_emotion.response" = "Condition * Statement Emotionality Rating")) %>%
      column_to_rownames(var = "Effect")
  
  table <- kbl(df, booktabs = T, digits = c(2,3,2,2,2,2,3), escape = F)
  
  return(table)
}
```


# Introduction {#sec-intro}

Utilized for communication in a wide variety of contexts, scatterplots are simple
representations of (usually) bivariate data. They were estimated in 1983 to account
for between 70 and 80 percent of data visualizations in scientific publications [@tufte_1983],
and while there is no doubt that the range of data visualizations employed in and
beyond science is now far broader, scatterplots remain an important tool for the
visualization designer. Evidence that scatterplots are interpreted rapidly [@rensink_2014]
facilitates the quick collection of large amounts of data, and their ubiquity
[@tufte_1983] and low levels of interindividual variance [@kay_2015] make 
them particularly suitable for studying perceptual and cognitive phenomena
regarding data visualization.

While most commonly used to communicate the linear correlation, or level of relatedness,
between a pair of variables, scatterplots can also be designed to facilitate the
detection of outliers, to convey differences between clusters, or to display
non-linear correlations. The suitability of scatterplots to such a range of tasks,
and the opportunity for designers to design with multiple tasks in mind, plays
a large part in their popularity. Building on previous work, we elect to focus
on the use of scatterplots for the communication of linear, bivariate, positive correlation.
There is evidence that, while correlation perception in scatterplots experiences
low levels of interindividual variance (especially when compared to other visualizations
that communicate the same idea [@harrison_2014; @kay_2015]), our accuracy in interpretation
is poor. Studies asking participants to numerically estimate correlation 
[@strahan_1978; @bobko_1979; @cleveland_1982; @lane_1985; @lauer_1989; @collyer_1990;
@meyer_1992] or estimate it via a bisection task [@rensink_2017] find consistent
levels of underestimation, particularly when 0.2 < *r* < 0.6. If scatterplots
were used solely for communication between those trained in statistics and data
visualization, this would not be particularly problematic, however this is not
the case; lay people are expected to be able to use and interpret data visualizations
on an almost daily basis. It is thus the duty of those who design such visualizations
to design with the naive, inexperienced viewer in mind. Doing so requires us to 
understand *how* visualizations work, and to gain an appreciation for the hidden processes
that allow pictorial representations to convey more than words and numbers
ever could.

Recent work has sought to address the correlation underestimation bias in
positively correlated scatterplots through the use of novel point encodings. In 
2023 and 2024, Strain et al. [@strain_2023; @strain_2023b; @strain_2024]
exploited the notion that viewers use the width of a probability distribution 
conveyed by the arrangement of scatterplot points as a proxy for their judgements
of correlation to successfully correct for the underestimation bias.
At the time of writing, this work has only provided evidence about perceptual
effects using a simple direct estimation paradigm, and while successful, has not 
investigated whether these techniques can influence cognition in the context
of real-world data visualizations and the relatedness between variables. As
doing this is crucial for providing designers with the tools to design visualizations
that work as intended in the field, we therefore present a two experiment 
study investigating the propensity for recently
established scatterplot visualization techniques to bias participants' beliefs
about the levels of relatedness between variables. 

# Related Work {#sec-rel-work-main}

In this section we briefly discuss related work on correlation perception and 
estimation, the history and current state of the use of point size and opacity
adjustments in scatterplots, including how these visual features have been used 
to correct for the underestimation bias, and perception and cognition
in data visualization. We then review the literature around belief change
as it pertains to data visualization.

## Correlation Perception {#sec-corr-percept}

Correlation describes the level of relatedness between a number of variables. There
are different types of correlation, however in this work, we use the
term to refer to Pearson's *r*, which takes a positive or negative value between
0 and 1 depending on the direction of the relationship being described. Mechanistically,
evidence is inconclusive regarding what drives correlation perception in scatterplots,
however some experimental results point towards the shape of the underlying probability
distribution as a likely candidate. Scatterplots with smaller point clouds produce
increased judgements of correlation [@cleveland_1982], suggesting that it is the area
of the point cloud that may influence perception. Work exploring the relationship
between subjective and objective *r* values in scatterplots found that this relationship
could be described by a power function that included the mean of the geometrical
distances between scatterplot points and a regression line [@meyer_1997]. Other work includes
some representation of the shape of a scatterplot's point cloud in equations
describing magnitude estimation and correlation discrimination [@meyer_1997;@rensink_2017],
and work on visual features as proxies for correlation found that a similar
quantity again is predictive of performance on correlation judgement tasks [@yang_2019]. While we cannot say that this *is*
the process of correlation perception in scatterplots, we can conclude that the 
shape of the point cloud is a good proxy for what is really occurring during
judgements of correlation. The goal of the work presented in this paper is not to further illuminate
what drives the perception of correlation in scatterplots, but rather to investigate
whether proven perceptual effects can be extended into a cognitive space. 

<!-- For this -->
<!-- reason, we do not explore these mechanisms further, but rather refer the reader -->
<!-- to previous literature which focuses on these questions [@rensink_2010; @rensink_2017; @strain_2023; @strain_2024]. -->

## Scatterplots: Opacity, Size, and Recent Developments {#sec-scatterplots}

Changing the opacities and sizes of points in scatterplots are standard practices.
Regarding opacity, this is often uniformly lowered to address overplotting
issues that arise when visualizing very large datasets [@matejka_2015]. Similarly, scatterplots
describing large datasets tend to have smaller points to maintain individual point
discriminability. Point size has also been used to encode an additional third
variable in what are known as *bubble charts*. Despite these techniques being
established, there is relatively little experimental work on the effects of 
changing point opacities and sizes on correlation estimation. Some studies have found
that correlation estimation is invariant to changes in point opacities and sizes
[@rensink_2014; @rensink_2017], while more recent work reports strong effects
of the systematic adjustment of each visual feature [@strain_2023; @strain_2023b;
@strain_2024]. The idea that it is the shape of the point cloud, and the probability distribution it
represents that is being used to inform judgements of correlation has received
support from recent work exploiting visual features with the intention of making
correlation estimation more accurate. Strain et al. [@strain_2023; @strain_2023b;
@strain_2024] changed the sizes and opacities of points
in scatterplots as a function of their distance from the regression line, and
achieved success in biasing correlation estimates in positive and negative
directions. When point opacities [@strain_2023] or point sizes [@strain_2023b]
were reduced with residual distance, participants were significantly more accurate
on a correlation estimation task; employing both of these manipulations simultaneously
[@strain_2024] resulted in an overshoot of correction, biasing participants
further. @fig-previous-manipulations contains a summary of previously tested
scatterplot manipulations and their effects on performance on a correlation 
estimation task. In those works, the opacities and sizes of scatterplot points are changed using
equation 1:

\begin{equation}
  point_{size/opacity} = 1 - b^{residual}
\end{equation}

In order to facilitate comparison, and as the aim of our study
is not to investigate the use of equation 1, rather the potential for alternative
designs to have effects on cognition, we utilize the same protocol here to produce
the stimuli for our main study, including the number of points (n = 128),
the value of *b* (0.25), and the size scaling factor and opacity floor.
For our atypical scatterplot condition, we choose
to use the previously established manipulation that has demonstrated the most dramatic
change in participants' estimates of correlation: a combination of point opacity
and size adjustments [@strain_2024] (see the right pair of plots in @fig-previous-manipulations).
While this manipulation has not produced the most accurate estimates of correlation
in previous work, we choose it to give ourselves the greatest chance of finding an effect,
should one exist.

```{r}
#| label: fig-previous-manipulations
#| fig-env: "figure*"
#| include: true
#| out-width: "100%"
#| fig-asp: 0.5
#| fig-cap: "Top row: Examples of scatterplot manipulations from previous work using an \\textit{r} value of 0.6. Bottom row: the corresponding correlation estimation behaviour across values of \\textit{r} between 0.2 and 0.99. The dashed diagonal line represents perfect estimation, while the solid line is what is observed when participants are asked to estimate correlation."

# dataframe containing values from previous work and the current is included
# in the data folder 

# set facet orders

facet_order <- c("standard_plot", "opacity_manipulated", "size_manipulated", "additive_manipulation")

# make data frame of behaviours observed with standard plot

standard_alone <- read_csv("data/all_exp.csv") %>%
    drop_na() %>%
    filter(factor == "standard_plot") %>%
    group_by(factor, my_rs) %>% 
    summarise(sd = sd(difference), mean = mean(difference)) %>%
    arrange(my_rs) %>%
    mutate(group = ceiling(row_number() / 2)) %>%
    group_by(group) %>%
    summarise(
      factor = first(factor),
      my_rs = mean(my_rs),
      sd = mean(sd),
      mean = mean(mean)
    ) %>%
  ungroup() %>%
  select(-group) %>%
  mutate(slider.response = (log(1-0.88*my_rs)/log(1-0.88)))

all_exp_df <- read_csv("data/all_exp.csv") %>%
    drop_na() %>%
    filter(factor != "standard_plot") %>%
    group_by(factor, my_rs) %>%
    mutate(factor = factor(factor, levels = facet_order))

plotting_df <- rbind(all_exp_df, standard_alone)

plot_est_prev <- function(filter) {
  
  plot <- plotting_df %>%
    filter(factor == filter) %>%
    ggplot(aes(x = my_rs, y = slider.response)) + 
    theme_ggdist() +
    scale_y_continuous(breaks = seq(-0.4,1, 0.2)) +
    theme(strip.text = element_text(size = 6, margin = margin(1,0,1,0, "mm")), aspect.ratio = 1,
          axis.text = element_text(size = 7),
          axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          plot.margin = unit(c(0,0,0.8,0), "cm")) +
    geom_abline(slope = 1, intercept = 0, linetype = 3) +
    labs(x = "Objective *r") +
    geom_smooth(se = FALSE, colour = "black", size = 0.4) +
    xlim(0.2,1) +
    ylim(0.1,1)
  
  return(plot)
}

plot_est_prev_no_y <- function(filter) {
  
  plot <- plotting_df %>%
    filter(factor == filter) %>%
    ggplot(aes(x = my_rs, y = slider.response)) + 
    theme_ggdist() +
    theme(scale_y_continuous(breaks = seq(-0.4,1, 0.2)),
        axis.text = element_text(size = 7),
        axis.title.x = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y = element_blank(),
        axis.ticks.y = element_blank(),
        plot.margin = unit(c(0,0.2,0,0), "cm")) +
    geom_abline(slope = 1, intercept = 0, linetype = 3) +
    labs(x = "Objective *r") +
    geom_smooth(se = FALSE, colour = "black", size = 0.4) +
    xlim(0.2,1) +
    ylim(0.1,1)
  
  return(plot)
}

example_plots_prev <- ggarrange(
                      plot_example_function(slopes,
                      (0.05),
                      (1),
                      "Standard Scatterplot"),
                      plot_example_function(slopes,
                      (0.05),
                      (1-slopes$slope_0.25),
                      "Opacity Manipulation"),
                      plot_example_function(slopes,
                      (1-slopes$slope_0.25),
                      (1),
                      "Size Manipulation"),
                      plot_example_function(slopes,
                      (1-slopes$slope_0.25),
                      (1-slopes$slope_0.25),
                      "Both Manipulations"), nrow = 1)

est_prev <- ggarrange(
  plot_est_prev_no_y("standard_plot"),
  plot_est_prev_no_y("opacity_manipulated"),
  plot_est_prev_no_y("size_manipulated"),
  plot_est_prev_no_y("additive_manipulation"), nrow = 1
)

p <- ggarrange(example_plots_prev, est_prev, nrow = 2) +  annotate("line", x = 0, end = 1, y = 0.5, yend = 0)

x_label <- text_grob(expression("Objective " * italic("r") * " value"))
y_label <- text_grob(expression("Subjective " * italic("r") * " value"),
                     rot = 90, hjust = 0.9)

#rm(example_plots_prev, plot_prev_est_no_y,    # trying to prevent env clogging
 #  plot_prev_est, plotting_df,                # up when working on paper
  # all_exp_df, standard_alone,
   #facet_order)

annotate_figure(p,
                bottom = x_label,
                left = y_label)
```


## Perception & Cognition in Data Visualization {#sec-perception-cognition}

Interacting with data visualization is a complex process involving bottom-up and
top-down mechanisms [@shah_2011; @franconeri_2021; @xiong_2022]. Previous work 
investigating alternative scatterplot designs has confined itself to exploring
perception only; here we introduce the potential for top-down effects to bias
participants. The influence of these cognitive factors is not our object of study, so we 
take measures to ensure that they are minimized, namely by crowdsourcing the creation
of our stimuli and by including by-participant effects in our modelling. Nevertheless,
we acknowledge that we introduce the potential for uncertainty regarding our conclusions; we 
argue this is necessary to gain a deeper understanding of how we interact
with data visualizations and how designers can improve their craft. 

## Persuasion & Belief Change in Data Visualization {#sec-persuasion}

Data visualization is a powerful tool. After all, if numerical data were sufficient
for understanding, there would be no need to visualize data beyond aesthetic 
preference. Pattern recognition, attention, and familiarity are all aspects of human
perception and cognition that can be exploited by visualization designers to 
facilitate more efficient, enjoyable, and effective communication [@franconeri_2021].
This is a double-edged sword, however; poor design, be it malevolent
or misguided, can cause distrust, confusion, and misunderstanding amongst viewers.
It is for these reasons that we choose to study belief change in scatterplots
as a consequence of alternative designs. Scatterplots, like many other data 
visualizations, have been submitted as evidence in court cases [@bobko_1979], and
play key roles in organizational decision making, including in healthcare [@poly_2019].
It is a reasonable to assume that data visualizations
are used to make decisions that result in positive or negative outcomes
with regards to health and policy more generally, especially given findings that, in certain
contexts, they are more persuasive than textual information [@pandey_2014]. Studying the potential for new
designs to alter beliefs about relatedness facilitates not only the exploitation
of these new designs for better visualizations, but also allows us to understand
how these designs might be used by malevolent actors with a view to inoculating
those who engage with them. To this end, we present a two
experiment study. First, we use crowdsourcing to select part of our experimental
stimuli, then we test the propensity for previously established alternative scatterplot
designs to alter beliefs about relatedness, taking into account a range of additional
participant qualities.

# General Methods {#sec-general-methods}

In this section we discuss our general research methods, including our implementations
of open research practices and our approach to and justification for crowdsourcing.

## Open Research {#sec-open-research}

Both our pre- and main studies were conducted according to the principles of open
and reproducible research [@ayris_2018]. We pre-registered hypotheses and analysis plans
with the Open Science Framework (OSF) for the pre-study[^1] and the main experiment[^2].
All data and analysis code are included in a GitHub repository[^3]. This repository 
contains instructions for building a Docker container [@merkel_2014] that reproduces the computational environment the paper
was written in. This allows for full replication of stimuli, figures, analyses,
and the paper itself. Ethical approval was granted by the (removed for anon).

[^1]: removed for anon
[^2]: removed for anon
[^3]: removed for anon

## Crowdsourcing {#sec-crowdsourcing}

While much prior work into correlation perception in scatterplots has taken place
in person, there is precedent for work that explores cognition and perception to take place
online using crowdsourced participants [@xiong_2022]. Crowdsourcing not only 
affords us recruitment of samples from across our lay population of interest,
it is considerably quicker and less expensive than in-person testing.
Previous work has reported issues of data quality
and skewed demographics [@chmielewski_2020; @charalambides_2021; @peer_2021], so
we follow published guidelines [@peer_2021] to give us the best chance of collecting
high quality data. We use the Prolific.co platform [@prolific] with strict pre-screening
criteria; participants were required to have completed at least 100 studies
using Prolific, and were required to have a Prolific score of 100, representing a 99%
approval rate.

# Pre-Study: Investigating Beliefs About Relatedness Statements {#sec-pre-study}

The goal of the present study is to investigate to what extent a novel scatterplot
design can alter participants' beliefs about the level of relatedness between variables.
There is evidence that belief change can be affected by prior beliefs and attitudes
[@xiong_2022; @markant_2023], and that emotion, including the content of a visualization
[@phelps_2006; @harrison_2013] and the emotional state of a participant [@thoresen_2016]
can have perceptual and cognitive effects on participants. We were unable
to find resources for correlative statements that included ratings for belief strength
and statement emotionality, so elected to create our own. In order
to control for these factors as much as possible, we ran our pre-study with the intent
of finding a correlative statement that was matched on emotional content and
level of belief strength. Instead of creating these statements ourselves, we chose
to streamline the process by using the ChatGPT4 Large Language Model [@chat_gpt]. We used
the following prompt:

```{=tex}

\begin{quotation}

    ``Generate 100 statements that describe the correlation between two variables, such as :

     "X is associated with a higher level of Y" or

     "As X increases, Y increases".

    Try to match all the statements on emotionality.``
    
\end{quotation}
```

The full list of these statements can be found in the supplementary materials.
Two authors rated each statement on emotionality and
strength of relatedness using Likert scales from 1 to 7. Both statement emotionality
and strength of relatedness were anchored at points 1 and 7: *Very Negative* and
*Very Positive* for the former, and *Not Related At All* and *Strongly Related* for
the latter. All other points were unlabelled. We calculated a quadratic weighted Cohen's Kappa
between the two raters using the **irr** package (version 0.84.1 [@irr])
in order to penalize larger magnitude disagreements more harshly.
We found agreement above chance for both statement emotionality
($\kappa$ = `r printnum(irr_emot$value, digits = 2)`, *p* `r printp(irr_emot$p.value, add_equals = TRUE)`)
and strength of correlation ($\kappa$ = `r printnum(irr_corr$value, digits = 2)`,
*p* `r printp(irr_corr$p.value, add_equals = TRUE)`), indicating moderate 
levels of agreement in both cases [@cohen_1968; @fleiss_1969]. Following this,
we selected strongly and weakly correlated statements with the highest level of
absolute agreement, resulting in 14 strongly correlated
statements and 11 weakly correlated statements that can be seen in the supplementary materials.
We then tested these 25 statements with a representative UK sample in order
to ascertain consensus on both statement emotionality and strength of relatedness.
Doing so allows us to control for these factors when we analyse the effects
of atypical scatterplot design on the propensity for belief change
in our main experiment. We hypothesized that:

  - H1: there will be a significant difference in average ratings of emotionality between statements. 
  - H2: there will be a significant difference between average ratings of strength of relatedness 
between statements.

```{r}
#| label: tbl-pre-test-hi
#| include: false
#| tbl-cap: Pre-test statements that were rated as being strongly correlated.

# read in pre-test csv data

pre_test_statements_hi <- read_csv("data/pre_test_data.csv") %>%
  filter(label == "high_corr") %>%
  select(c("item_no","statement")) %>%
  rename("Statement - Strong Correlation Depicted" = "statement",
         "Item Number" = "item_no")

# make table

kbl(pre_test_statements_hi, booktabs = T)
```

```{r}
#| label: tbl-pre-test-low
#| include: false
#| tbl-cap: Pre-test statements that were rated as being weakly correlated.

# read in pre-test csv data

pre_test_statements_lo <- read_csv("data/pre_test_data.csv") %>%
  filter(label == "low_corr") %>%
  select(c("item_no","statement")) %>% 
  rename("Statement - Weak Correlation Depicted" = "statement",
         "Item Number" = "item_no")

# make table

kbl(pre_test_statements_lo, booktabs = T)
```

## Method {#sec-method-pre}

### Participants {#sec-participants-pre}

100 participants were recruited using the Prolific.co platform [@prolific]. English fluency and
UK residency was required for participation, as our main experiment relied on familiarity
with data visualizations from a popular British news source. In addition to 25
experimental items, we included six attention check items asking participants
to ignore the scatterplot and provide specific answers. No participants failed more than 2 out of 6 attention
check items, and therefore data from all 100 were included in the full analysis
(`r printnum(gender_pre$Male, digits = 0)`% male and `r printnum(gender_pre$Female, digits = 0)`% 
female). Participants' mean age was `r printnum(age_pre$mean, digits = 1)` (*SD* = 
`r printnum(age_pre$sd, digits = 1)`). The average time taken to complete the survey was
`r printnum(time_taken_pre$mean, digits = 1)` minutes (*SD* = `r printnum(time_taken_pre$sd, digits = 1)` minutes).

### Design {#sec-design-pre}

Each participant saw all survey items (see supplementary material),
along with the six attention check items, in a fully randomized order. All
experimental code, materials, and instructions are hosted on GitLab[^4].

[^4]: removed for anon

### Procedure {#sec-procedure-pre}

The experiment was built using Psychopy [@pierce_2019] and hosted on Pavlovia.org.
Participants were permitted to complete the experiment using a phone, tablet, desktop,
or laptop computer. Participants were first shown the participant information sheet
and were asked to provide consent through key presses in response to consent statements.
They were asked to provide their age in a free text box, followed by their 
gender identity. Participants were told that they would be asked to read statements
about the relatedness between a pair of variables, after which they would have to 
indicate their beliefs about statement emotionality and the strength of relatedness suggested
using a pair of sliders. To familiarize themselves with the sliders, they were asked to complete a practice round
in response to the statement: "As participation in online experiments increases,
society becomes happier." The Likert scales used in the online experiment were
identical to those described in @sec-pre-study.

## Results {#sec-results-pre}

```{r}
#| label: kappa-pre-test

# Calculate Fleiss' Kappa for 100 raters on emotional valence

pre_test_emot <- beliefs_scatterplots_pretest_tidy %>%
  select(c("participant", "slider_emotion.response", "item_no"))

emot_matrix <- xtabs(slider_emotion.response ~ item_no + participant, data = pre_test_emot)

emot_matrix <- as.matrix(emot_matrix)

emot_kappa <- kappam.fleiss(emot_matrix)

# do the same for strength of belief

pre_test_belief <- beliefs_scatterplots_pretest_tidy %>%
  select(c("participant", "slider_belief.response", "item_no"))

belief_matrix <- xtabs(slider_belief.response ~ item_no + participant, data = pre_test_belief)

belief_matrix <- as.matrix(belief_matrix)

belief_kappa <- kappam.fleiss(belief_matrix)

# calculate mean ratings and standard deviations for each statement

agreement_df_emot <- beliefs_scatterplots_pretest_tidy %>%
  group_by(item_no) %>%
  summarise(mean_emot = mean(slider_emotion.response),
            std_dev_emot = sd(slider_emotion.response)) %>%
  arrange(std_dev_emot)

agreement_df_belief <- beliefs_scatterplots_pretest_tidy %>%
  group_by(item_no) %>%
  summarise(mean_belief = mean(slider_belief.response),
            std_dev_belief = sd(slider_belief.response)) %>%
  arrange(std_dev_belief)

ratings_df <- full_join(agreement_df_belief, agreement_df_emot, by = "item_no")

# calculate average topic emotionality ratings

avg_emot <- ratings_df %>%
  filter(between(mean_emot, 3, 5))

# calculate consensus by summing standard deviations

consensus_df <- avg_emot %>%
  mutate(consensus = std_dev_belief + std_dev_emot) %>% 
  arrange(consensus) %>%
  slice_head(n =  2)
```

All analyses were conducted using R (version `r paste0(R.version$major, ".", R.version$minor)`).
We use the **irr** package to calculate Fleiss' Kappa to measure interrater agreement
on statement emotionality and strength of relatedness for the 25 experimental items.
This analysis revealed that participants agreed above chance on statement emotionality
($\kappa$ = `r printnum(emot_kappa$value, digits = 2)`, *p* `r printp(emot_kappa$p.value, add_equals = TRUE)`)
and strength of correlation ($\kappa$ = `r printnum(belief_kappa$value, digits = 2)`,
*p* `r printp(belief_kappa$p.value, add_equals = TRUE)`).

## Selecting Statements for the Main Experiment {#sec-selecting-statements}

To control for the potential effects of statement emotionality in the main experiment,
we first select statements that represent neutral emotional valence. Statements
with average emotionality ratings between 3 and 5 are statements 
`r printnum(add_and_to_numbers(avg_emot$item_no), digits = 0)`. To ascertain
which statements represent the greatest consensus, we add standard deviations in
ratings for statement emotionality and strength of relatedness. Due to concerns about experimental
power, and in line with evidence that propensity for belief change is highest
when prior beliefs are not strongly held [@xiong_2022; @markant_2023],
we elected at this point to test only the statement corresponding to weak beliefs
about the strength of correlation between the variables in question. We therefore
test statement number `r printnum(add_and_to_numbers(consensus_df$item_no[2]), digits = 0)`,
"Higher consumption of spicy foods is associated with a lower risk of certain types of cancer",
however we modify the wording so that the variables (food consumption and cancer risk)
are positively correlated, as while the manipulations we use
in the atypical scatterplot condition are able to change estimates of correlation
in positively correlated scatterplots, no work regarding the effects of these
manipulations in negatively correlated scatterplots has been completed.

## Discussion {#sec-discussion-pre}

Fleiss' Kappa values for interrater agreement on both statement emotionality
and strength of correlation scales are low ($\kappa$ = `r printnum(emot_kappa$value, digits = 2)`
and $\kappa$ = `r printnum(belief_kappa$value, digits = 2)` respectively),
however do exceed that which would be expected by chance. We suggest this may be
due to Fleiss' Kappa not being designed with ordinal (Likert scales in this case) data in mind.
In light of this we do not make decisions regarding which statement to use based 
on the values of Fleiss' Kappa observed, but rather on the standard deviations of 
ratings across all raters. Regardless, we do not consider this to be a particular
weakness, as we also test statement emotionality and strength of correlation with participants
in the main study and include these ratings as part of our analysis.

# Main Study: Potential for Belief Change Using Atypical Scatterplots {#sec-main-study}

We test the statement that exhibited the lowest average level of belief about
correlation and the 2nd highest level of consensus. Modified for directionality,
this statement is therefore: "Higher consumption of plain (non-spicy) foods
is associated with a lower risk of certain types of cancer." To give ourselves
the best chance of detecting an effect of viewing atypical scatterplots, we elected
to design our scatterplots based on a popular British news source and to falsely
credit the data as being provided by the British National Health Service (NHS). Participants
were informed that said news source had requested that their identity were obscured,
and were debriefed that this was not the case, and that the data were fictional,
following completion of the experiment. We hypothesized that:

  - H1: there will be a significant difference in ratings of strength of
  relatedness before and after participants viewed the experimental items.
  - H2: that this difference will be greatest when participants are exposed
  to scatterplots in the atypical condition.

### Defensive Confidence {#sec-def-con}

In line with evidence that those who are more confident in their ability to defend
their own positions are more susceptible to having those positions changed [@albarracin_2004],
we test participants' defensive confidence using a 12-item scale. This scale is 
replicated from previous work in the supplemental material, and has additionally
been utilized more recently [@markant_2023] to explore the potential for attitude
change specifically with regards to correlations in scatterplots. Participants
provide answers to the 12 scale items using a 5 point Likert scale anchored at points 1
(*not at all characteristic of me*) and 5 (*extremely characteristic of me*), with 
all other points being unlabelled. Analysis including participants' defensive 
confidence scores is included in @sec-add-analyses-discussion.

## Stimuli {#sec-stimuli-main}

Having selecting a correlative statement describing a weak relationship and with
a high level of consensus between participants in the pre-study, we used **ggplot2**
(version 3.5.1 [@ggplot]) in R to create our stimuli; scripts for the creation
of stimuli can be found in the repository associated with this project.
As our statement was rated as describing a low level of relatedness,
we utilize scatterplots that describe a strong relationship (0.6 > *r* > 0.99)
in order to induce belief change. Our plots
were created in line with guidance provided by previous research and detailed in
@sec-scatterplots. We used 45 values of *r* uniformly distributed between 0.6
and 0.99 to create 45 scatterplots for each condition.
Examples of stimuli using an *r* value of 0.6 for both the
typical and atypical scatterplot conditions can be seen in @fig-main-examples.

```{r}
#| label: fig-main-examples
#| include: true
#| fig-env: "figure*"
#| out-width: "100%"
#| fig-cap: Examples of the experimental stimuli used with an \textit{r} value of 0.6.

# function for making slopes df

slope_function <- function(my_desired_r) {
  set.seed(1234)
  
  my_sample_size = 128
  
  mean_variable_1 = 5
  sd_variable_1 = 1
  
  mean_variable_2 = 76
  sd_variable_2 = 5
  
  mu <- c(mean_variable_1, mean_variable_2) 
  
  myr <- my_desired_r * sqrt(sd_variable_1) * sqrt(sd_variable_2)
  
  mysigma <- matrix(c(sd_variable_1, myr, myr, sd_variable_2), 2, 2) 
  
  corr_data = as_tibble(mvrnorm(my_sample_size, mu, mysigma, empirical = TRUE))
  
  corr_model <- lm(V2 ~ V1, data = corr_data)
  
  my_residuals <- abs(residuals(corr_model))
  
  data_with_resid <- round(cbind(corr_data, my_residuals), 2)
  
  slopes <- data_with_resid %>%
    mutate(slope_0.25 = 1-(0.25)^my_residuals) %>%
    mutate(slope_inverted = (1 + (0.25)^ my_residuals)-1) %>%
    mutate(slope_inverted_floored = pmax(0.2,(1+(0.25)^my_residuals)-1)) %>%
    mutate(typical = 0.033) %>%
    mutate(standard_alpha = 1)
  
  return(slopes)
}

# manually specify variables from slopes df

slopes <- slope_function(0.6)
slopeI <- (slopes$slope_inverted)
slopeI_floored <- (slopes$slope_inverted_floored)
typical <- (slopes$typical)
standard_alpha <- (slopes$standard_alpha)

# function for creating example plots

example_plot_function <- function(slopes, my_desired_r, size_value, opacity_value, theme) {
  
  p <- ggplot(slopes, aes(x = V1, y = V2)) +
    scale_size_identity() +
    scale_alpha_identity() +
    geom_point(aes(size =  6*(size_value + 0.3), alpha = opacity_value), shape = 16) +  
    geom_hline(yintercept = 68, size = 1, colour="#333333") +
    geom_segment(x = 0, xend = 10, y = 66.2, yend = 66.2, size = 0.3, colour="#585858") +
    bbc_style() +
    theme(axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          title = element_text(size = 14),
          plot.subtitle = element_text(size = 9),
          plot.caption = element_text(size = 9, hjust = -0.02),
          plot.margin = unit(c(0,0,1,0), "cm")) +
    labs(title = "Spicy Foods",
         subtitle = "Higher consumption of plain (non-spicy) foods\nis associated with a higher risk of certain types of cancer.",
         caption = "Source: NHS England") +
    annotate("text", x = 3, y = 67, label = "Less plain Diet") +
    annotate("text", x = 7, y = 67, label = "More plain Diet") +
    annotate("text", x = 1, y = 71, label = "Fewer Cancer\nDiagnoses", angle = 90) +
    annotate("text", x = 1, y = 80, label = "More Cancer\nDiagnoses", angle = 90) +
    coord_cartesian(clip = "off")
  
  return(p)
  
}

# use ggarrange to arrange plots, include annotation geoms for plot titles

example_plots <- ggarrange(example_plot_function(slopes, 0.6, typical, standard_alpha, bbc_style()),
                           example_plot_function(slopes, 0.6, slopeI, slopeI_floored, bbc_style()),nrow = 1) +
  annotate(geom = "text",
           label = "Typical Scatterplot",
           x = 0.25,
           y = 0.04,
           size = 7) +
  annotate(geom = "text",
           label = "Atypical Scatterplot",
           x = 0.75,
           y = 0.04,
           size = 7) +
    annotate(geom = "segment",
           x = 0.077,
           xend = 0,
           y = 0.04,
           yend = 0.04,
           size = 0.6,
           colour = "grey",
           arrow = arrow(length = unit(0.15, "cm"), angle = 90)) +
  annotate(geom = "segment",
           x = 0.935,
           xend = 1,
           y = 0.04,
           yend = 0.04,
           size = 0.6,
           colour = "grey",
           arrow = arrow(length = unit(0.15, "cm"), angle = 90)) +
      annotate(geom = "segment",
           x = 0.43,
           xend = 0.5,
           y = 0.04,
           yend = 0.04,
           size = 0.6,
           colour = "grey",
           arrow = arrow(length = unit(0.15, "cm"), angle = 90)) +
  annotate(geom = "segment",
           x = 0.565,
           xend = 0.5,
           y = 0.04,
           yend = 0.04,
           size = 0.6,
           colour = "grey",
           arrow = arrow(length = unit(0.15, "cm"), angle = 90))

rm(slopeI, slopeI_floored, typical, standard_alpha, slopes) # clean up

example_plots
```

## Method {#sec-method-main}

### Participants {#sec-participants-main}

Participants were recruited using Prolific.co [@prolific]. English fluency and UK
residency was required for participation, as well as normal or corrected-to-normal
vision, and having not participated in any of our previous studies regarding 
correlation perception in scatterplots [refs removed for anon]. Data were
collected from 77 participants for each condition. 2 participants failed more than
2 out of 4 attention check questions for each condition, meaning their data were
excluded per pre-registration stipulations. Data from the remaining 150 participants
were included in the full analysis (`r printnum(gender_main$Male, digits = 1)`% male,
`r printnum(gender_main$Female, digits = 1)`% female, and `r printnum(gender_main$'Non-binary', digits = 1)`% non-binary).
Participants' mean age was `r printnum(age_main$mean, digits = 1)`
(*SD* = `r printnum(age_main$sd, digits = 1)`). Participants' mean graph literacy
score was `r printnum(literacy$mean, digits = 1)` (*SD* = `r printnum(literacy$sd, digits = 1)`)
out of 30, their mean defensive confidence score was `r printnum(def_con$mean, digits = 1)`
(*SD* = `r printnum(def_con$sd, digits = 1)`) out of 60, and their mean rating of 
statement emotionality was `r printnum(topic_emo$mean, digits = 1)`
(*SD* = `r printnum(topic_emo$sd, digits = 1)`) on a 7 point Likert scale.
On average, participants took `r printnum(time_taken_main$mean, digits = 1)`
minutes to complete the experiment (*SD* = `r printnum(time_taken_main$sd)`).

### Design {#sec-design-main}

We employed a between-participants design. Each participant was randomly assigned
to either group A, in which case they viewed typical scatterplots, or group B,
in which they viewed atypical scatterplots designed deliberately to elicit higher
levels of belief change. Participants saw all 45 experimental items for their group, along
with 4 attention check items, in a fully randomized order. Our dependent variable 
was the level of belief change induced by viewing the scatterplot visualizations,
so participants were tested on how strongly related they believed the variables 
described by the correlative statement were **before** and **after** viewing the
experimental items. All experimental code, materials, and instructions are 
hosted on GitLab as two separate experiments [^5] [^6]

[^5]: removed for anon
[^6]: removed for anon

### Procedure {#sec-procedure-main}

We use PsychoPy [@pierce_2019] to build our experiment and Pavlovia.org
to host it. Participants were permitted to complete the experiment on a desktop
or laptop computer. We elected to prevent participants from using a phone or
tablet to complete the experiment as there is evidence that differences in the on-screen sizes
of data visualizations can alter perceptions [@cleveland_1982]. Participants were
first shown the participant information sheet and asked to provide consent
through key presses in response to consent statements. They were, again,
asked to provide their age and gender identity. Participants then completed the 12-item
Defensive Confidence scale described by Albarracín and Mitchell [@albarracin_2004]
and the 5-item Subjective Graph Literacy scale [@garcia_2016] [^7].
In order to give legitimacy to our data visualizations with the hope of maximizing
any potential belief change, participants were told that the graphs were taken
from a well-known British news source, but that the identity of this source had 
been obscured. In order to promote engagement with the visualizations,
participants were instructed to use a slider to estimate the correlation displayed
in each scatterplot; no hypotheses were made based on these data and therefore we
do not analyse them further. Following instructions, which included descriptions of scatterplots and Pearson's *r*,
participants had a chance to practice using the slider, before being 
asked to indicate their belief about emotionality and the relatedness between variables
described in our chosen statement. We captured these data using Likert scales
identical to those described in @sec-pre-study. After completing the experimental
trials, participants were tested again on their beliefs about relatedness, and 
then debriefed that the data they were shown were fictional. Interspersed among
the experimental items were 4 attention check trials which explicitly
asked participants to set the slider to 0 or 1.

[^7]: The inclusion of this scale was not specified in the pre-registration.

## Results {#sec-results-main}

```{r}
#| label: model-rating-time
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# create model

rating_time_model <- buildclmm(rating ~ rating_time +
                         (1  | participant),
                       data = main_exp_tidy)

# linear mixed effects model (metric), see footnote 8
#
# if you want to run this, recode the rating factor as unordered
#
# LMM <- buildmer(rating ~ rating_time +
#                         (1  | participant),
#                       data = main_exp_tidy)
```

```{r}
#| label: model-cmpr-rating-time
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# build comparison model with fixed effects term removed

rating_time_model_cmpr <- comparison(rating_time_model)
```

```{r}
#| label: anova-rating-time

# run ANOVA between experimental and comparison models, outputs stats in global env

anova_results_mixed(rating_time_model, rating_time_model_cmpr)
```

All analyses were conducted using R (version `r paste0(R.version$major, ".", R.version$minor)`).
Likert scales capture whether one rating is higher or lower than another, however
they do not quantify the difference between levels of rating. Metric modelling
assuming equal levels of difference between ratings, such as linear regression, 
is therefore inappropriate [@liddell_2018]. In light of this, we use the **ordinal**
package (version 2023.12-4.1 [@ordinal]) to build cumulative link mixed effects
models to analyse Likert scale data [^8]. We use the **buildmer** (version 2.11 [@buildmer])
package to automate the selection of the random effects structure; we provide a
maximal model, which includes random intercepts for participants, and **buildmer**
identifies the most complex model that successfully converges. We present odds ratios and equivalent
Cohen's *d* effect sizes that were calculated using the **effectsize** package
(version 0.8.9 [@effectsize]). 

[^8]: Our pre-registration specified linear mixed effects models (metric). Conclusions
are identical when using said models, and code for this analysis is included in the repository
associated with this paper.

To test the first hypothesis,
that ratings of strength of relatedness would be different before and after participants
viewed experimental items, we build a model whereby the rating of strength of relatedness
the participant made is predicted by whether it was made **before** or **after** viewing 
the experimental items. Our first hypothesis was supported; there was a 
significant difference in ratings of strength of relatedness made before and after
participants viewed the experimental plots. A likelihood ratio test revealed that the model
including time of rating as a predictor explained significantly more variance than
the null ($\chi^2$(`r in_paren(rating_time_model.df)`) = `r printnum(rating_time_model.LR)`,
*p* `r printp(rating_time_model.p, add_equals = TRUE)`). This model has random
intercepts for participants. Statistical testing providing
support for this hypothesis is shown in @tbl-rating-time. @fig-descriptives
shows means and boxplots for ratings of strength of relatedness before and after
viewing scatterplots in either the typical or atypical condition.

```{r}
#| label: tbl-rating-time
#| include: true
#| tbl-cap: "Statistics for the significant main effect of rating time. Odds ratio and the equivalent Cohen's \\textit{d} value is also supplied."

make_sig_table_abso(rating_time_model)
```

```{r}
#| label: fig-descriptives
#| include: true
#| out-width: "100%"
#| fig-asp: 0.5
#| fig-cap: Boxplots showing ranges, interquartile ranges, medians (vertical lines) and means for participants' ratings of strength of relatedness before and after viewing either typical or atypical scatterplots.

main_exp_tidy %>%
  mutate("condition" = recode(condition,
                            "A" = "Atypical\nScatterplot",
                            "T" = "Typical\nScatterplot"),
         "rating_time" = recode(rating_time,
                                "pre_bel" = "Pre",
                                "post_bel" = "Post")) %>%
  mutate(rating = as.numeric(rating)) %>%
  ggplot(aes(x = condition, y = rating, fill = rating_time)) +
  geom_boxplot(outliers = F, key_glyph = "polygon") +
  scale_color_brewer(palette = "Dark2", aesthetics = "fill") +
  stat_summary(fun.y = mean, geom = "point", shape = 15, size = 4, position = position_dodge(0.75)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 7)) +
  labs(x = "",
       y = "Rating",
       fill = "Rating Time") +
  theme_ggdist() +
  theme(legend.position = c(0.1, 0.1),
        legend.background = element_rect(fill = NA),
        legend.direction = "horizontal",
        legend.box = "horizontal",
        legend.key.size = unit(1., "line"),
        legend.title.position = "top",
        legend.title = element_text(size = 10),
        axis.line.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_text(size = 11, colour = "black"),
        axis.text.x = element_text(size = 11)) +
  guides(shape = F) +
  guides(fill = guide_legend(override.aes = list(shape = NA), reverse = T)) + 
  coord_flip() 
```

```{r}
#| label: model-condition-interact
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# modelling effect of condition on difference between belief strength before and
# after viewing plot

condition_interact_model <- buildclmm(rating ~ rating_time*condition +
                              (1 | participant),
                            data = main_exp_tidy)

# linear mixed effects model (metric), see footnote 8
#
# if you want to run this, recode the rating factor as unordered
#
# LMM <- buildmer(rating ~ rating_time*condition +
#                         (1  | participant),
#                       data = main_exp_tidy)
```

```{r}
#| label: model-condition-interact-cmpr
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# build null model for comparison

condition_interact_model_cmpr <- comparison(condition_interact_model)
```

```{r}
#| label: model-condition-interact-results
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# extract ANOVA results for relative vs null model comparison

anova_results_mixed(condition_interact_model, condition_interact_model_cmpr)
```

Our second hypothesis, that the difference between ratings of strength of relatedness
before and after viewing experimental plots would be greater when participants
were assigned to the atypical scatterplot condition, also received support.
Deviation coding was used for each of the experimental factors of rating time (pre- or post-)
and condition, which allows us to compare means of ratings to the grand mean. We built
a cumulative link mixed effects model whereby the rating of strength of relatedness the participant
made was predicted by the condition the participant was assigned to and the time
they made the rating. A likelihood ratio test revealed that the model
including condition and rating time as a predictor explained significantly more variance than the null
($F$(`r in_paren(condition_interact_model.df)`) = `r printnum(condition_interact_model.LR)`,
*p* `r printp(condition_interact_model.p, add_equals = TRUE)`). This model had random
intercepts for participants. We again found a main effect of rating time,
found no main effect of condition, and found an interaction between
rating time and condition. Test statistics, along with odds ratios and equivalent 
Cohen's *d* can be seen in @tbl-condition-interact. To further explore the interaction, we used the **emmeans**
package (version 1.10.4 [@emmeans]) to calculate pairwise comparisons between 
levels of the condition and rating time factors, which can be see in @tbl-pairwise.
The interaction we found is driven by participants beliefs changing more between
pre-viewing and post-viewing times for atypical as opposed to typical plots.

```{r}
#| label: tbl-condition-interact
#| include: true
#| tbl-cap: Statistics for the significant main effect of condition on the difference between pre- and post- scatterplot viewing ratings for typical and atypical plots. Odds ratios and equivalent Cohen's *d* are also shown. NB the odds ratio for the effect of condition is caluclated based on the absolute value of the estimate. 

make_sig_table_rel(condition_interact_model)

# get estimate

#summary <- summary(relative_model)

#coef <- summary$coefficients

#condition_estimate <- coef[2]

#rm(summary, coef)
```

```{r}
#| label: tbl-pairwise
#| include: true 
#| tbl-cap: Pairwise comparisons. The interaction is driven by there being greater differences in ratings of belief strength made before and after viewing plots in the atypical condition compared to the typical.

contrast_table_df <- contrasts_extract(condition_interact_model) %>%
  mutate(p.value = scales::pvalue(p.value)) %>%
  rename("\\textit{p}" = "p.value",
         "Z ratio" = "Z.ratio") %>%
  mutate('Contrast' = recode(Contrast,
         "post_bel A - pre_bel A" = "Post-Viewing x Atypical | Pre-Viewing x Atypical",
         "pre_bel A - pre_bel T" = "Pre-Viewing x Atypical | Pre-Viewing x Typical",
         "pre_bel A - post_bel T" = "Pre-Viewing x Atypical | Post-Viewing x Typical",
         "post_bel A - pre_bel T" = "Post-Viewing x Atypical | Pre-Viewing x Typical",
         "post_bel A - post_bel T" = "Post-Viewing x Atypical | Post-Viewing x Typical",
         "post_bel T - pre_bel T" = "Post-Viewing x Typical | Pre-Viewing x Typical"))

kbl(contrast_table_df, booktabs = TRUE, digits = c(0,2,3), escape = FALSE)
```

### Additional Analyses {#sec-add-analyses}

```{r}
#| label: add-analyses
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

lit_model <- clmm(rating ~ 1 + condition + rating_time + condition:rating_time * literacy +
                         (1  | participant), 
          data = main_exp_tidy)

anova_results_mixed(lit_model, condition_interact_model)

dc_model <- clmm(rating ~ 1 + condition + rating_time + condition:rating_time * dc_score +
                         (1  | participant), 
          data = main_exp_tidy)

anova_results_mixed(dc_model, condition_interact_model)

emo_model <- clmm(rating ~ 1 + condition + rating_time + condition:rating_time * slider_emotion.response +
                         (1  | participant), 
          data = main_exp_tidy)

anova_results_mixed(emo_model, condition_interact_model)
```

We find effects of participants' scores on a defensive confidence test 
($F$(`r in_paren(dc_model.df)`) = `r printnum(dc_model.LR)`,
*p* `r printp(dc_model.p, add_equals = TRUE)`), participants' scores on a graph
literacy test ($F$(`r in_paren(lit_model.df)`) = `r printnum(lit_model.LR)`,
*p* `r printp(lit_model.p, add_equals = TRUE)`), and of how emotional participants
rated the chosen correlative statement before beginning the block of trials
($F$(`r in_paren(emo_model.df)`) = `r printnum(emo_model.LR)`,
*p* `r printp(emo_model.p, add_equals = TRUE)`). We discuss the interactions
between the main effect and  graph literacy, defensive confidence, and statement emotionality in
@sec-add-analyses-discussion.

## Discussion {#sec-main-discussion}

Both of our hypotheses were supported in this experiment. Participants reliably 
updated their beliefs after viewing scatterplots, and the difference between 
pre- and post-viewing beliefs was greatest for the participants who viewed scatterplots
in the atypical condition. These results suggest that the perceptual effects described
in previous work can be extended to bias participants' cognitions about the levels
of relatedness between pairs of variables. These findings are encouraging for 
data visualization designers who wish to design scatterplots such that correlation
perception more closely matches the underlying statistics, however much more work
would be required to begin producing guidelines for the use of alternative 
scatterplot designs with regards to producing more persuasive visualizations.

### Graph Literacy, Defensive Confidence, and Statement Emotionality {#sec-add-analyses-discussion}

```{r}
#| label: fig-add-analyses-plots
#| include: true
#| fig-asp: 0.4
#| fig-cap: Illustrating how differences in beliefs about strength of relatedness change as a function of participants' scores on the graph literacy test (Left), their scores on the defensive confidence test (Middle), and their ratings of statement emotionality (Right). Localled smoothed cruves are shown separately for typical and atypical scatterplot viewing conditions. Lower ratings of Difference in Belief ($y$ axis) correspond to lower levels of belief change between pre- and post- scatterplot viewing times.

add_analyses_plotting_df <- main_exp_tidy %>%
  mutate("condition" = recode(condition,
                              "A" = "Atypical",
                              "T" = "Typical")) %>%
  pivot_longer(cols = c("literacy", "dc_score", "slider_emotion.response"),
               names_to = "add_variable",
               values_to = "variable_score") %>%
  mutate("add_variable" = recode(add_variable,
                                 "literacy" = "Graph Literacy (/30)",
                                 "dc_score" = "Defensive Confidence (/60)",
                                 "slider_emotion.response" = "Emotion Rating (1 to 7)")) %>%
  mutate(add_variable = as_factor(add_variable))

ggplot(aes(x = variable_score, y = belief_diff, colour = condition), data = add_analyses_plotting_df) +
    geom_textpath(stat = "smooth",
            method = "loess",
            span = 1,
            vjust = -0.3,
            hjust = 1,
            size = 3.5,
            aes(colour = condition, label = condition)) +
  scale_color_brewer(palette = "Set2", aesthetics = "color") +
  theme_ggdist() +
  ggh4x::facet_wrap2(~factor(add_variable, c("Graph Literacy (/30)",
                                             "Defensive Confidence (/60)",
                                             "Emotion Rating (1 to 7)")), scales = "free_x") +
  theme(legend.position = "none",
        axis.title.y = element_text(size = 12),
        strip.text = element_text(size = 8)) +
  labs(x = "",
       y = "Difference in Belief") +
  ggh4x::facetted_pos_scales(
    x = list(
      scale_x_continuous(breaks = scales::pretty_breaks(n = 7)),
      scale_x_continuous(limits = c(20,60)),
      scale_x_continuous(breaks = scales::pretty_breaks(n = 7)) 
    )
  )
```

Mean differences in pre- and post- plot-viewing ratings of strength of relatedness
by Subjective Graph Literacy score can be seen in @fig-add-analyses-plots. Generally, 
participants who scored higher on a graph literacy test experienced smaller changes
in their ratings of strength of relatedness. While the effect size associated 
with this interaction is small (~ 0.01), it is in line with
previous work suggesting that those with higher levels of graph or visualization
literacy show better performance in inference tasks related to visualizations 
[@canham_2010], are more capable of describing effects that visualizations aim
to communicate [@shah_2011], and are able to preferentially attend to relevant 
features of visualizations to a greater degree [@okan_2016], than those with 
lower levels of graph literacy. In the present study, we provide evidence that
those with greater levels of graph literacy are *less susceptible* to having 
their beliefs changed by visualizations, although this difference is somewhat
reduced if participants viewed atypical scatterplots, in which case levels
of belief change were relatively consistent.

We observe an opposing pattern of results when examining the effects of
defensive confidence on participants' propensity for belief change. Generally,
participants who scored more highly on the defensive confidence test experienced
greater levels of belief change. This is in line with evidence that those who 
are more confident in their ability to defend their own beliefs are more liable
to having those beliefs changed in light of evidence [@albarracin_2004]. This
effect has previously been explained as being due to those with a greater degree
of confidence in their own ability to defend their ideas engaging with information
with lower levels of attention to the fact it opposes their beliefs. The present 
study provides additional evidence in favour of this phenomenon. While the general
pattern of results is expected based on previous work, the interaction present 
between defensive confidence and scatterplot condition is novel (see @fig-add-analyses-plots).
It would appear that despite following the normal pattern of results for
low to moderate levels of defensive confidence, those participants who viewed
the typical scatterplots experienced a drop in belief change as defensive 
confidence increased past ~ 36/60. We suggest that the unfamiliar nature of 
the atypical scatterplots was protective against an unexpected, standard behaviour
whereby very high levels of defensive confidence decrease susceptibility to belief change. 

The effect of statement emotionality on belief change is also illustrated in
@fig-add-analyses-plots. There is a broad research space regarding emotionality and
data visualization [@lan_2023], and it is clear from previous work that emotion
affects perception, cognition, and behaviour [@phelps_2006; @harrison_2013; @thoresen_2016]
with regard to data visualization. Harrison et al. [@harrison_2013] found that 
participants who were positively primed performed better on a low-level
visual judgement task. Comparison of this work to the current is difficult, as
*success* on our task is hard to define.

Further experimental work is required to provide concrete explanations for the
interactive effects of graph literacy, defensive confidence, and statement
emotionality in the current experimental paradigm, as the investigation of
these is not the prime goal of the present study.

# General Discussion {#sec-general-discussion}

The most parsimonious explanation for the results we observe in the present study
is as follows; things that *look* more related will be *judged* as being more related.
As simple and obvious as this sounds, testing it empirically is a necessary step
in broadening the data visualization design space and bringing novel designs
closer towards real-life use cases while maintaining a strong foundation of experimental
evidence. Having controlled as far as possible for factors such as the emotional
content of the graph, the general consensus on how related the variables in question
were, and the general design (bar the points themselves) of the scatterplot,
we can conclude with strong evidence that atypical scatterplot design was responsible
for increasing the level of belief change amongst participants.

Previous work has provided support for the idea that it is the shape of the
point cloud, more specifically, the width of the probability distribution it
represents, that drives correlation perception in scatterplots. Our present
results do not allow us to comment further on the particularities of that mechanism;
if this mechanism were valid, however, our results would be expected. These 
results are broadly consequential. For data visualization designers, they provide
strong evidence that utilizing alternative scatterplot designs described here
and in previous work can affect beliefs about levels of relatedness without
requiring the removal of data. For researchers, these results pave the way for work
in a number of directions, which we discuss in @sec-future-work.

# Future Work {#sec-future-work}

Because alternative scatterplot designs have not been tested before with regards
to belief change, we elected to design our study with the intention of capturing
effects, should they exist. This meant that our design was simple; we did not investigate
multiple correlative statements, the propensity for strongly held beliefs to 
be changed, nor the effect of topics with strong or polarized emotional
components. We chose to use a simple, blunt measure of belief about relatedness,
and did not investigate variations within the alternative scatterplot design space,
such as using different values of equation 1, or using opacity or size manipulations in isolation.
Each of these components deserves study, and each is ripe for future work to
investigate the contributions of each factor to the effects we have seen here.

In @sec-add-analyses-discussion, we describe the effects that graph literacy,
defensive confidence, and participants' ratings of the emotionality of the correlative
statement have on the propensity for belief change. Future work may wish to investigate these factors,
along with others, such as educational background or spatial abilities that also
affect perceptions of correlation [@tandon_2024]. Xiong et al.
[@xiong_2022] describe how correlation estimation may differ according to the
context the data are presented in; this could be extended to instead investigate statements with
differing emotional contents and how alternative scatterplot designs might interact
with emotional valence. Similarly, selecting matched participant groups with
low or high graph literacy or defensive confidence would facilitate understanding 
of how we might use alternative designs to cater for people with different levels of experience,
or who differ in terms of their faith in their own ideas and abilities.

Previous work investigating beliefs with regards to correlation estimation have made
distinctions between beliefs and attitudes [@xiong_2022; @markant_2023]. We elected
not to do so due to our utilization of alternative designs. Markant et al. [@markant_2023]
found that while beliefs about correlations changed in participants as a result of
interacting with scatterplots, attitudes did not. Future work may wish to investigate
whether this finding would persist with scatterplots utilizing the alternative
designs described here. Finally, while changing perceptions, beliefs, and attitudes
are promising early steps, changing people's behaviours would be the real test
of the power of alternative visualization techniques; while this may be difficult
to study, future work should investigate whether what we have found here may 
be used to induce behaviour change.

# Limitations {#sec-limitations}

Our commitment to finding an effect, should one exist, is also our biggest
limitation. The exploratory nature of the work results in us being unable to 
comment specifically on how different forms of size and opacity manipulation in 
scatterplots may change beliefs in different ways, although addressing this using
the framework we present here would be simple to accomplish. To this date, there
has been no qualitative work performed on alternative scatterplot designs such
as those we utilize here; it may be that any perceptual or cognitive benefits
are outweighed by distrust or unfamiliarity with novel designs. We chose to use 
a simple, blunt, 7-point Likert scale. While we argue that this is not particularly
problematic given that we intended only to find an effect (and succeeded in that),
future work may wish to use techniques that provide further scope for analysis,
such as the graphical elicitation method developed by Karduni et al. [@karduni_2021;
@karduni_2023].

# Conclusion {#sec-conclusion}

We investigated the effects of alternative scatterplot designs that vary the opacities
and sizes of points as a function of their distance from the regression line on 
the propensity for belief change following visualization viewing. We presented these
designs, and corresponding typical scatterplots, as if they were part of a news item
and used a real-world variable pair that had been selected by our population of interest
as being representative of a weakly-held, emotionally neutral correlation. We found 
that participants who viewed scatterplots employing alternative designs
experienced greater levels of belief change than those who viewed typical
plots. In addition, we found small interactive effects of a number of participant
characteristics. Our results suggest that visualization techniques that have
previously been employed to improve perception amongst participants are deserving
of study with regards to their potential to change beliefs.

# References {.unnumbered}
