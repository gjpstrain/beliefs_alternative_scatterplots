---
format: acm-pdf

# use keep-tex to cause quarto to generate a .tex file
# which you can eventually use with TAPS

keep-tex: true

params:
  eval_models: true
  
knitr:
  opts_chunk: 
    cache_comments: false
    crop: true
    
execute: 
  echo: false
  warning: false
  message: false
  include: false

bibliography: atypical-scatterplots.bib

title: Changing Beliefs About Correlations in Atypical Scatterplots

# if short-title is defined, then it's used
short-title: 

author:
  - name: Gabriel Strain
    email: gabriel.strain@manchester.ac.uk
    orcid: 0000-0002-4769-9221
    affiliation:
      name: Department of Computer Science, Faculty of Science and Engineering, University of Manchester
      address: Oxford Road
      city: Manchester
      country: United Kingdom
      postal-code: M13 9PL
  - name: Andrew J. Stewart
    email: andrew.j.stewart@manchester.ac.uk
    affiliation:
      name: Department of Computer Science, Faculty of Science and Engineering, University of Manchester
      address: Oxford Road
      city: Manchester
      country: United Kingdom
      postal-code: M13 9PL
  - name: Paul Warren
    email: paul.warren@manchester.ac.uk
    affiliation:
      name: Division of Psychology, Communication and Human Neuroscience, School of Health Sciences, Faculty of Biology, Medicine, and Health, University of Manchester
      address: Oxford Road
      city: Manchester
      country: United Kingdom
      postal-code: M13 9PL
  - name: Charlotte Rutherford
    email: charlotte.rutherford-2@postgrad.manchester.ac.uk
    affiliation:
      name: Division of Psychology Communication and Human Neuroscience, School of Health Sciences, Faculty of Biology, Medicine, and Health, University of Manchester
      address: Oxford Road
      city: Manchester
      country: United Kingdom
      postal-code: M13 9PL
  - name: Caroline Jay
    affiliation:
      name: Department of Computer Science, Faculty of Science and Engineering, University of Manchester
      address: Oxford Road
      city: Manchester
      country: United Kingdom
      postal-code: M13 9PL

# acm-specific metadata
acm-metadata:
  # comment this out to make submission anonymous
  # anonymous: true

  # comment this out to build a draft version
  final: false

  # comment this out to specify detailed document options
  # acmart-options: sigconf, review  

  # acm preamble information
  copyright-year: 2018
  acm-year: 2018
  copyright: acmcopyright
  doi: XXXXXXX.XXXXXXX
  conference-acronym: "Conference acronym 'XX"
  conference-name: |
    Make sure to enter the correct
    conference title from your rights confirmation emai
  conference-date: June 03--05, 2018
  conference-location: Woodstock, NY
  price: "15.00"
  isbn: 978-1-4503-XXXX-X/18/06

  # if present, replaces the list of authors in the page header.
  shortauthors: Strain et al.

  # The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
  # Please copy and paste the code instead of the example below.
  ccs: |
    \begin{CCSXML}
    <ccs2012>
     <concept>
      <concept_id>10010520.10010553.10010562</concept_id>
      <concept_desc>Computer systems organization~Embedded systems</concept_desc>
      <concept_significance>500</concept_significance>
     </concept>
     <concept>
      <concept_id>10010520.10010575.10010755</concept_id>
      <concept_desc>Computer systems organization~Redundancy</concept_desc>
      <concept_significance>300</concept_significance>
     </concept>
     <concept>
      <concept_id>10010520.10010553.10010554</concept_id>
      <concept_desc>Computer systems organization~Robotics</concept_desc>
      <concept_significance>100</concept_significance>
     </concept>
     <concept>
      <concept_id>10003033.10003083.10003095</concept_id>
      <concept_desc>Networks~Network reliability</concept_desc>
      <concept_significance>100</concept_significance>
     </concept>
    </ccs2012>
    \end{CCSXML}
    
    \ccsdesc[500]{Computer systems organization~Embedded systems}
    \ccsdesc[300]{Computer systems organization~Redundancy}
    \ccsdesc{Computer systems organization~Robotics}
    \ccsdesc[100]{Networks~Network reliability}

  keywords:
    - belief change
    - correlation perception
    - scatterplot
    - crowdsourced
  
abstract: |
  abstract goes here

---

```{r}
#| label: setup

set.seed(1234) # random seed for number generation

library(tidyverse)
library(MASS)
library(emmeans)
library(scales)
library(buildmer)
library(lme4)
library(kableExtra)
library(papaja)
library(qwraps2)
library(lmerTest)
library(ggdist)
library(ggpubr)
library(conflicted)
library(ggtext)
library(r2glmm)
library(grid)
library(DescTools)
library(Matrix)
library(irr)
library(bbplot)
library(geomtextpath)

# fix conflicts now using the conflicted package

conflicts_prefer(dplyr::select(), dplyr::filter(), lme4::lmer(), DescTools::AUC())

options(knitr.kable.NA = "")

```

```{r}
#| label: lazyload-cache

if (!params$eval_models){ lazyload_cache_dir("beliefs_attitudes_atypical_scatterplots_cache/pdf") }
```

```{r}
#| label: load-data

# load in data csvs for pre-test and main experiment, remove the "__participant" column now 

pre_test_anon <- read_csv("data/pre_test_response_final.csv") %>% select(-"__participant")

main_exp_A <- read_csv("data/main_test_A_final.csv") %>% select(-"__participant")

main_exp_T <- read_csv("data/main_test_T_final.csv") %>% select(-"__participant")
```

```{r}
#| label: wrangle-data

## NB: With the exception of anonymization, data are provided as-is from 
## pavlovia (survey tool). Wrangling functions *must* be run first to make
## the data sets usable

wrangle_pre_test <- function(anon_file) {
  
# extract demographic information
  
demographics <- anon_file %>%
  filter(!is.na(gender_slider.response)) %>%
  select(matches(c("participant",
                   "age_textbox.text",
                   "gender_slider.response")))

time_taken <- anon_file %>%
  filter(!is.na(final.stopped)) %>%
  select(c("participant", "final.stopped"))
  

# select relevant columns
# select only experimental items
# change data types where appropriate
# output this file with suffix 'tidy'

anon_file %>%
  select(c("participant",
           "avg_corr",
           "avg_emot",
           "slider_emotion.response",
           "slider_belief.response",
           "statement",
           "item_no",
           "label",
           "session",
           )) %>%
  filter(item_no < 26) %>%
  full_join(demographics, by = "participant") %>%
  full_join(time_taken, by = "participant") %>%
  assign(paste0(unique(anon_file$expName), "_tidy"),
           value = ., envir = .GlobalEnv)
}

wrangle_main_exp <- function(anon_file) {
  
# extract demographic information
  
demographics <- anon_file %>%
  filter(!is.na(gender_slider.response)) %>%
  select(matches(c("participant",
                   "age_textbox.text",
                   "gender_slider.response")))

# extract time taken information

time_taken <- anon_file %>%
  filter(!is.na(final.stopped)) %>%
  select(c("participant", "final.stopped"))

# extract defensive confidence testing scores

defensive_confidence <- anon_file %>%
  select(contains(c("DC", "participant"))) %>% 
  select(-contains(c("stopped", "started"))) %>%
  rename(DC_1 = q1_slider_DC.response,
         DC_2 = q2_slider_DC.response,
         DC_3 = q3_slider_DC.response,
         DC_4 = q4_slider_DC.response,
         DC_5 = q1_slider_DC_2.response,
         DC_6 = q2_slider_DC_2.response,
         DC_7 = q3_slider_DC_2.response,
         DC_8 = q4_slider_DC_2.response,
         DC_9 = q1_slider_DC_3.response,
         DC_10 = q2_slider_DC_3.response,
         DC_11 = q3_slider_DC_3.response,
         DC_12 = q4_slider_DC_3.response) %>%
  mutate(DC_3 = 6 - DC_3,                # reverse score items 3, 4, 10, 12
         DC_4 = 6 - DC_4,
         DC_10 = 6 - DC_10,
         DC_12 = 6 - DC_12) %>%
  group_by(participant) %>%
  summarise(
    dc_score = sum(DC_1,
                   DC_2,
                   DC_3,
                   DC_4,
                   DC_5,
                   DC_6,
                   DC_7,
                   DC_8,
                   DC_9,
                   DC_10,
                   DC_11,
                   DC_12,
                   na.rm = T)
  )

# extract literacy info

literacy <- anon_file %>%
    filter(!is.na(q5_slider.response)) %>%
    rowwise() %>%
    mutate(literacy = sum(c(q1_slider.response, 
                            q2_slider.response, 
                            q3_slider.response, 
                            q4_slider.response, 
                            q5_slider.response))) %>%
    select(participant,
           literacy)

# split unique item no column into number (dataset used) and letter (condition)

anon_file <- anon_file %>%
  separate(starts_with("unique"), into = c("item_no", "condition"), sep = "(?<=\\d)(?=\\D)")

# extract pre belief, post belief, and belief_diff

belief <- anon_file %>%
  group_by(participant) %>%
  summarise(
    pre_bel = sum(slider_belief.response, na.rm = T),
    post_bel = sum(slider_belief_post.response, na.rm = T)
  ) %>%
  mutate(belief_diff = post_bel - pre_bel) %>%
  select(participant, belief_diff, pre_bel, post_bel)

emotion <- anon_file %>%
  filter(!is.na(slider_emotion.response)) %>%
  group_by(participant) %>%
  select(participant, slider_emotion.response)

# select relevant columns
# select only experimental items
# change data types where appropriate
# output this file with suffix 'tidy'

anon_file %>%
  select(c("participant",
           "item_no",
           "condition",
           "slider.response",
           "trials.thisN"
           )) %>%
  mutate(half = case_when(
    trials.thisN < 23 ~ "first",
    trials.thisN > 23 ~ "second")) %>%
  filter(item_no < 46) %>%
  inner_join(literacy, by = "participant") %>%
  inner_join(demographics, by = "participant") %>%
  inner_join(belief, by = "participant") %>%
  inner_join(emotion, by = "participant") %>%
  inner_join(defensive_confidence, by = "participant") %>%
  full_join(time_taken, by = "participant") %>%
  mutate(across(matches(c("item_no", "condition")), as_factor)) %>%
  mutate(trials.thisN = as.integer(trials.thisN)) %>%
  assign(paste0(unique(anon_file$expName), "_tidy"),
           value = ., envir = .GlobalEnv)

}

# use wrangling function on anonymized data files

wrangle_pre_test(pre_test_anon)

wrangle_main_exp(main_exp_A)

wrangle_main_exp(main_exp_T)

# add 75 to each participant number for second part of main exp

atypical_scatterplots_main_test_T_tidy <- atypical_scatterplots_main_test_T_tidy %>% mutate(participant = participant + 75)

# main experiment is between participants, so rbind main_exp dfs together

main_exp_tidy <- rbind(atypical_scatterplots_main_test_A_tidy,
                       atypical_scatterplots_main_test_T_tidy)

# set deviation coding for experimental model

contrasts(main_exp_tidy$condition) <- matrix(c(.5, -.5))


main_exp_tidy <- main_exp_tidy %>%
  mutate(dc_section = case_when(
    dc_score < 20 ~ "low",
    dc_score > 20 & dc_score < 40 ~ "average",
    dc_score > 40 ~ "high"
  ))

# check for missing age and gender id values

sum(is.na(beliefs_scatterplots_pretest_tidy$gender_slider.response))

sum(is.na(beliefs_scatterplots_pretest_tidy$age_textbox.text))

# missing age and gender values for a single participant
# print session with missing values

beliefs_scatterplots_pretest_tidy %>%
  filter(is.na(age_textbox.text)) %>%
  distinct(session)

# session id was used to look up age and gender identity and manually add
# this information to the final results csv using the prolific-supplied
# demographic information for the pretest
# this is not included as would expose personally identifiable prolific IDs

# Extract gender data into separate dfS

gender_pre <- distinct(beliefs_scatterplots_pretest_tidy, participant,
                      .keep_all = TRUE) %>%
  group_by(gender_slider.response) %>%
  summarise(perc = n()/nrow(.)*100) %>%
  pivot_wider(names_from = gender_slider.response, values_from = perc)

gender_main <- distinct(main_exp_tidy, participant,
                      .keep_all = TRUE) %>%
  group_by(gender_slider.response) %>%
  summarise(perc = n()/nrow(.)*100) %>%
  pivot_wider(names_from = gender_slider.response, values_from = perc)

# extract age data into separate dfS

age_pre <- distinct(beliefs_scatterplots_pretest_tidy, participant,
                .keep_all = TRUE) %>%
  summarise(mean = mean(age_textbox.text, na.rm = TRUE),
            sd = sd(age_textbox.text, na.rm = TRUE))

age_main <- distinct(main_exp_tidy, participant,
                .keep_all = TRUE) %>%
  summarise(mean = mean(age_textbox.text, na.rm = TRUE),
            sd = sd(age_textbox.text, na.rm = TRUE)) 

# extract time taken data into separate dfs

time_taken_pre <- distinct(beliefs_scatterplots_pretest_tidy, participant,
                .keep_all = TRUE) %>%
  summarise(mean = mean(final.stopped/60, na.rm = TRUE),
            sd = sd(final.stopped/60, na.rm = TRUE)) 

time_taken_main <- distinct(main_exp_tidy, participant,
                .keep_all = TRUE) %>%
  summarise(mean = mean(final.stopped/60, na.rm = TRUE),
            sd = sd(final.stopped/60, na.rm = TRUE))

# extract literacy data into separate df

literacy <- distinct(main_exp_tidy, participant,
                        .keep_all = TRUE) %>%
  summarise(mean = mean(literacy), sd = sd(literacy))

# extract defensive confidence data into separate df

def_con <- distinct(main_exp_tidy, participant,
                        .keep_all = TRUE) %>%
  summarise(mean = mean(dc_score), sd = sd(dc_score))

# extract topic emotionality data into separate df

topic_emo <- distinct(main_exp_tidy, participant,
                        .keep_all = TRUE) %>%
  summarise(mean = mean(slider_emotion.response), sd = sd(slider_emotion.response))

# remove all intermediate dfs

rm(main_exp_A,
   main_exp_T,
   atypical_scatterplots_main_test_A_tidy,
   atypical_scatterplots_main_test_T_tidy,
   pre_test_anon)
```

```{r}
#| label: format-numbers-function

# function to format numbers when papaja prints them

add_and_to_numbers <- function(numbers) {

  num_str <- as.character(numbers)

  len <- length(num_str)
  
  if (len == 1) {

    return(num_str)
    
  } else if (len == 2) {
    
    return(paste(num_str[1], "and", num_str[2]))
    
  } else {

    return(paste(paste(num_str[1:(len-1)], collapse = ", "), ", and ", num_str[len], sep = ""))
  }
}

```

```{r}
#| label: author-pre-test-ratings

# NB: this code is reproduced here so that test values are accessible
# It can also be found at item_preparation/pre_test_stim.R

# Create dataframes for each rater

author_A_statements <- read_csv("item_preparation/statements_A.csv") %>%
  rename(Topic_Emotionality_A = Topic_Emotionality,
         Strength_of_Correlation_A = Strength_of_Correlation) %>%
  select(-Notes)
  

author_B_statements <- read_csv("item_preparation/statements_B.csv") %>%
  rename(Topic_Emotionality_B = Topic_Emotionality,
         Strength_of_Correlation_B = Strength_of_Correlation) %>%
  select(-Notes)

# Bind dataframes together

all_ratings <- left_join(author_A_statements,
                         author_B_statements,
                         by = c("Number",
                                "Statements"))

## IRR Calculations

irr_emot <- kappa2(matrix(c(all_ratings$Topic_Emotionality_A,
                all_ratings$Topic_Emotionality_B), ncol = 2), "squared")    

irr_corr <- kappa2(matrix(c(all_ratings$Strength_of_Correlation_A,
                all_ratings$Strength_of_Correlation_B), ncol = 2), "squared")    

```

```{r}
#| label: comparison-function

# this function takes a model and creates a nested model with the fixed effects 
# terms removed for anova comparison

comparison <- function(model) {
  
  parens <- function(x) paste0("(",x,")")
  onlyBars <- function(form) reformulate(sapply(findbars(form),
                                              function(x)  parens(deparse(x))),
                                       response=".")
  onlyBars(formula(model))
  cmpr_model <- update(model,onlyBars(formula(model)))
  
  return(cmpr_model)
  
}
```

```{r}
#| label: anova-results-functions

# this function takes two nested models, runs an anova, and the outputs the 
# test statistic, the degrees of freedom, and the p value to the global 
# environment

anova_results_mixed <- function(model, cmpr_model) {
  
  model_name <- deparse(substitute(model))
  
  if (class(model) == "buildmer") model <- model@model
  if (class(cmpr_model) == "buildmer") cmpr_model <- cmpr_model@model
  
  anova_output <- anova(model, cmpr_model)
  
  assign(paste0(model_name, ".Chisq"),
         anova_output$Chisq[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".df"),
         anova_output$Df[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".p"),
         anova_output$`Pr(>Chisq)`[2],
         envir = .GlobalEnv)
  
}

anova_results_linear <- function(cmpr_model, model) {
  
  model_name <- deparse(substitute(model))
  
  if (class(model) == "buildmer") model <- model@model
  if (class(cmpr_model) == "buildmer") cmpr_model <- cmpr_model@model
  
  anova_output <- anova(cmpr_model, model)
  
  assign(paste0(model_name, ".F"),
         anova_output$'F'[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".df"),
         anova_output$Df[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".p"),
         anova_output$`Pr(>F)`[2],
         envir = .GlobalEnv)
  
}
```

```{r}
#| label: make-sig-table-lm

# function to create tables with fixed effects and interactions for LMM
# first do function for absolute differences model (H1)

make_sig_table_abso <- function (model) {
  
  # buildmer class models need reassigned to lmer class for further use
  
  if (class(model) == "buildmer") model <- model@model
  
  # subset model summary to get list of fixed/interaction effects
  # then make this a data frame, rename columns, fix p value formatting
  
  model_summary <- summary(model)
  
  coef_table <- model_summary$coefficients

  df_residual <- model_summary$df[2]
  df_model <- model_summary$df[1]
  
  coef_table <- cbind(coef_table, "df" = c(rep(df_model, nrow(coef_table) - 1), df_residual))

  df <- as_tibble(coef_table, rownames = "Effect") %>%
    rename("Standard Error" = "Std. Error",
           "t-value" = "t value",
    "p" = "Pr(>|t|)") %>%
    mutate(p = scales::pvalue(p)) %>%
    rename("\\textit{p}" = "p") %>%
    relocate(df, .after = "Standard Error") 
  
    r_squared <- r2beta(model, method = "nsj") %>%
    select(c("Effect", "Rsq")) %>%
    mutate(Rsq = round(Rsq, 3)) %>%
    mutate(Rsq = ifelse(row_number() == 1, "", Rsq)) %>%
    rename("R\\textsuperscript{2}" = "Rsq") 
    
    df <- left_join(df, r_squared, by = "Effect") %>%
      mutate("Effect" = recode(Effect,
                               "rating_timepre_bel" = "Rating Time")) %>%
      column_to_rownames(var = "Effect")
 
  table <- kbl(df, booktabs = T, digits = c(2,3,2,2,2,2,3), escape = F)
  
  return(table)
}

make_sig_table_rel <- function (model) {
  
  # buildmer class models need reassigned to lmer class for further use
  
  if (class(model) == "buildmer") model <- model@model
  
  # subset model summary to get list of fixed/interaction effects
  # then make this a data frame, rename columns, fix p value formatting
  
  model_summary <- summary(model)
  
  coef_table <- model_summary$coefficients

  df_residual <- model_summary$df[2]
  df_model <- model_summary$df[1]
  
  coef_table <- cbind(coef_table, "df" = c(rep(df_model, nrow(coef_table) - 1), df_residual))

  df <- as_tibble(coef_table, rownames = "Effect") %>%
    rename("Standard Error" = "Std. Error",
           "t-value" = "t value",
    "p" = "Pr(>|t|)") %>%
    mutate(p = scales::pvalue(p)) %>%
    rename("\\textit{p}" = "p") %>%
    relocate(df, .after = "Standard Error") 
  
    r_squared <- r2beta(model, method = "nsj") %>%
    select(c("Effect", "Rsq")) %>%
    mutate(Rsq = round(Rsq, 3)) %>%
    mutate(Rsq = ifelse(row_number() == 1, "", Rsq)) %>%
    rename("R\\textsuperscript{2}" = "Rsq") 
    
    df <- left_join(df, r_squared, by = "Effect") %>%
      mutate("Effect" = recode(Effect,
                               "condition1" = "Condition")) %>%
      column_to_rownames(var = "Effect")
 
  table <- kbl(df, booktabs = T, digits = c(2,3,2,2,2,2,3), escape = F)
  
  return(table)
}

make_sig_table_lit <- function (model) {
  
  # buildmer class models need reassigned to lmer class for further use
  
  if (class(model) == "buildmer") model <- model@model
  
  # subset model summary to get list of fixed/interaction effects
  # then make this a data frame, rename columns, fix p value formatting
  
  model_summary <- summary(model)
  
  coef_table <- model_summary$coefficients

  df_residual <- model_summary$df[2]
  df_model <- model_summary$df[1]
  
  coef_table <- cbind(coef_table, "df" = c(rep(df_model, nrow(coef_table) - 1), df_residual))

  df <- as_tibble(coef_table, rownames = "Effect") %>%
    rename("Standard Error" = "Std. Error",
           "t-value" = "t value",
    "p" = "Pr(>|t|)") %>%
    mutate(p = scales::pvalue(p)) %>%
    rename("\\textit{p}" = "p") %>%
    relocate(df, .after = "Standard Error") 
  
    r_squared <- r2beta(lit_model, method = "nsj") %>%
    select(c("Effect", "Rsq")) %>%
    mutate(Rsq = round(Rsq, 3)) %>%
    mutate(Rsq = ifelse(row_number() == 1, "", Rsq)) %>%
    rename("R\\textsuperscript{2}" = "Rsq") 
    
    df <- left_join(df, r_squared, by = "Effect") %>%
      mutate("Effect" = recode(Effect,
                               "condition1" = "Condition",
                               "literacy" = "Literacy",
                               "condition1:literacy" = "Condition * Literacy")) %>%
      column_to_rownames(var = "Effect")
  
  table <- kbl(df, booktabs = T, digits = c(2,3,2,2,2,2,3), escape = F)
  
  return(table)
}

make_sig_table_dc <- function (model) {
  
  # buildmer class models need reassigned to lmer class for further use
  
  if (class(model) == "buildmer") model <- model@model
  
  # subset model summary to get list of fixed/interaction effects
  # then make this a data frame, rename columns, fix p value formatting
  
  model_summary <- summary(model)
  
  coef_table <- model_summary$coefficients

  df_residual <- model_summary$df[2]
  df_model <- model_summary$df[1]
  
  coef_table <- cbind(coef_table, "df" = c(rep(df_model, nrow(coef_table) - 1), df_residual))

  df <- as_tibble(coef_table, rownames = "Effect") %>%
    rename("Standard Error" = "Std. Error",
           "t-value" = "t value",
    "p" = "Pr(>|t|)") %>%
    mutate(p = scales::pvalue(p)) %>%
    rename("\\textit{p}" = "p") %>%
    relocate(df, .after = "Standard Error") 
  
    r_squared <- r2beta(model, method = "nsj") %>%
    select(c("Effect", "Rsq")) %>%
    mutate(Rsq = round(Rsq, 3)) %>%
    mutate(Rsq = ifelse(row_number() == 1, "", Rsq)) %>%
    rename("R\\textsuperscript{2}" = "Rsq") 
    
    df <- left_join(df, r_squared, by = "Effect") %>%
      mutate("Effect" = recode(Effect,
                               "condition1" = "Condition",
                               "dc_score" = "Defensive Confidence",
                               "condition1:dc_score" = "Condition * Defensive Confidence")) %>%
      column_to_rownames(var = "Effect")
  
  table <- kbl(df, booktabs = T, digits = c(2,3,2,2,2,2,3), escape = F)
  
  return(table)
}

make_sig_table_emo <- function (model) {
  
  # buildmer class models need reassigned to lmer class for further use
  
  if (class(model) == "buildmer") model <- model@model
  
  # subset model summary to get list of fixed/interaction effects
  # then make this a data frame, rename columns, fix p value formatting
  
  model_summary <- summary(model)
  
  coef_table <- model_summary$coefficients

  df_residual <- model_summary$df[2]
  df_model <- model_summary$df[1]
  
  coef_table <- cbind(coef_table, "df" = c(rep(df_model, nrow(coef_table) - 1), df_residual))

  df <- as_tibble(coef_table, rownames = "Effect") %>%
    rename("Standard Error" = "Std. Error",
           "t-value" = "t value",
    "p" = "Pr(>|t|)") %>%
    mutate(p = scales::pvalue(p)) %>%
    rename("\\textit{p}" = "p") %>%
    relocate(df, .after = "Standard Error") 
  
    r_squared <- r2beta(model, method = "nsj") %>%
    select(c("Effect", "Rsq")) %>%
    mutate(Rsq = round(Rsq, 3)) %>%
    mutate(Rsq = ifelse(row_number() == 1, "", Rsq)) %>%
    rename("R\\textsuperscript{2}" = "Rsq") 
    
    df <- left_join(df, r_squared, by = "Effect") %>%
      mutate("Effect" = recode(Effect,
                               "condition1" = "Condition",
                               "slider_emotion.response" = "Topic Emotionality Rating",
                               "condition1:slider_emotion.response" = "Condition * Topic Emotionality Rating")) %>%
      column_to_rownames(var = "Effect")
  
  table <- kbl(df, booktabs = T, digits = c(2,3,2,2,2,2,3), escape = F)
  
  return(table)
}
```

# Introduction {#sec-intro-main}

Utilized for communication in a wide variety of contexts, scatterplots are simple
representations of (usually) bivariate data. They were estimated in 1983 to account
for between 70 and 80 percent of data visualizations in scientific publications [@tufte_1986],
and while there is no doubt that the range of data visualizations employed in and
beyond science is now far broader, scatterplots remain an important tool for the
data visualization designer. The appeal of researching scatterplots lies in evidence that
people generally interpret them in similar ways [@kay_2015], that they are interpreted
rapidly [@rensink_2014], and that they are ubiquitous in both academic [@tufte_1986]
and non-academic contexts.

While most commonly used to communicate the linear correlation, or level of relatedness,
between a pair of variables, scatterplots can also be designed to facilitate the
detection of outliers, to convey differences between clusters, or to display
non-linear correlations. The suitability of scatteplots to such a range of tasks,
and the opportunity for designers to design with a range of tasks in mind, plays
a large part in their popularity. Building on previous work, we elect to focus
on the use of scatterplots for the communication of linear, bivariate, positive correlation.
There is evidence that, while correlation perception in scatterplots experiences
low levels of interindividual variance (especially when compared to other visualizations
that communicate the same idea [@harrison_2014; kay_2015]), our accuracy in interpretation
is poor. Studies asking participants to numerically estimate correlation 
[@strahan_1978; @bobko_1979; @cleveland_1982; @lane_1985; @lauer_1989; @collyer_1990;
@meyer_1992] or estimate it via a bisection task [@rensink_2017] find consistent
levels of underestimation, particularly when 0.2 < *r* < 0.6. If scatterplots
were used solely for communication between those trained in statistics and data
visualization, this would not be especially problematic, however this is not
the case, with lay people being expected to be able to use and interpret data visualizations
on an almost daily basis. It is thus the duty of those who design such visualizations
to design with the naive, inexperienced viewer in mind. Doing so requires us to 
understand *how* visualizations work, and to gain an appreciation for the hidden processes
that allow pictorial representations to convey much more than words and numbers
ever could.

Recent work has sought to address the correlation underestimation bias in
positively correlated scatterplots through the use of novel point encodings. In 
2023 and 2024, Strain et al. [@strain_2023; @strain_2023b; @strain_2024] began
exploiting the idea that viewers use the width of a probability distribution 
conveyed by the arrangement of scatterplot points as a proxy for their judgements
of correlation to successfully (albeit partially) correct for the underestimation bias.
As of the time of writing, this work has only provided evidence about perceptual
effects using a simple direct estimation paradigm, and while successful, has not 
investigated whether these techniques can influence cognition in the context
of real-world data visualizations and the relatedness between variables. As
doing this is crucial for providing designers with the tools to design visualizations
that are adapted for the facilitation of more accurate correlation perception, we
therefore present a two experiment study investigating the propensity for recently
established scatterplot visualization techniques to bias participant's beliefs
about levels of relatedness between variables. 


- and has seen success by using point size and opacity manipulations
- as of writing, this work has only provided evidence about perceptual effects using a simple estimation paradigm
- and has not investigated whether these effects can extend to cognition about relatedness between real variables
- this is important wrt the potential for using the manipulations to improve visualisations
- we present a 2 experiment study in which we first use novel research methods and
crowdsourcing to select a statement about relatedness that is agreed upon (phrasing),
then we test the propensity for previously described visual manipulations to 
alter beliefs about relatedness, taking into account various additional participant qualities.

# Related Work {#sec-rel-work-main}

In this section we briefly discuss related work on correlation perception and 
estimation, the history and current state of the use of point size and opacity
adjustments in scatterplots, including how these visual features have been used 
with regards to correction for the underestimation bias, and perception and cognition
in data visualization. We then review the literature around belief change
as it pertains to data visualization, before ending with our hypotheses for the
present study.

- Correlation Perception
  Testing
  Drivers
- Scatterplots
  Opacity Adjustments in Scatterplots
  Point Size Adjustments in Scatterplots
- Perception & Cognition in Data Visualization
  Differences between them
  Why study the extension from one to the other?
  Beliefs changing as a result of viewing visualisations
    the persuasive power of data viz
    why this is important over just presenting numbers
    maybe something specifically about scatterplots
    lead onto the primary motivation of this study
  hypotheses- statement  
    

# General Methods {#sec-general-methods}

In this section we discuss our general research methods, including our implementations
of open research practices, our approach to and justification for crowdsourcing, and our 
use of the ChatGPT4 LLM in preparing parts of our stimuli. 

## Open Research {#sec-open-research}

Both our pre and main studies were conducted according to the principles of open
and reproducible research [@ayris_2018]. We pre-registered hypotheses and analysis plans
with the Open Science Framework (OSF) for the pre-study[^1] and the main experiment[^2], and 
there were no deviations from them. All data and analysis code are included in a 
GitHub repository[^3]. This repository contains instructions for building a Docker
container [@merkel_2014] that reproduces the computational environment the paper
was written in. This allows for full replication of stimuli, figures, analysis,
and the paper itself. Ethical approval was granted by the (removed for anon).

[^1]: removed for anon
[^2]: removed for anon
[^3]: removed for anon

## Crowdsourcing

While much prior work into correlation perception in scatterplots has taken place
in person, there is precedence for work that explores cognition to take place
online using crowdsourced participants [@xiong_2022]. Crowdsourcing not only 
affords us recruitment of samples from across our lay population of interest,
it is considerably quicker and less expensive than in-person testing. We therefore choose
to crowdsource all participants. Previous work has reported issues of data quality
and skewed demographics [@chmielewski_2020; @charalambides_2021; @peer_2021], so
we follow published guidelines [@peer_2021] to give us the best chance of collecting
high quality data. We use the Prolific.co platform [@prolific] with strict pre-screening
criteria; participants were required to have completed at least 100 studies
using Prolific, and were required to have a Prolific score of 100, representing a 99%
approval rate.

## Use of Large Language Models

 - issues regarding stimulus generation normally
 - advantages conferred by using ChatGPT
 - reproducibility issues?

# Pre-Study: Investigating Beliefs About Relatedness Statements {#sec-pre-study}

## Introduction {#sec-pre-study-intro}

### Testing Beliefs {#sec-testing-beliefs}

### Preparation of Stimuli {#sec-stim-prep-pre}

Due to previous evidence suggesting effects of prior belief strength and topic
emotionality on the propensity for belief change, we first aim to build a
picture of people's thoughts and feelings along these dimensions in our
population of interest. With the intention of testing the potential for
changes in beliefs about correlations displayed in scatterplots depicting
weak and strong correlations, and those whose topics were both strong and
neutral in emotional valence, we began by using ChatGPT4 [@chatgpt] to
generate 100 correlation statements using the following prompt:

```{=tex}

\begin{flushleft}

    ``Generate 100 statements that describe the correlation between two variables, such as :

     "X is associated with a higher level of Y" or

     "As X increases, Y increases".

    Try to match all the statements on emotionality.``
    
\end{flushleft}
```

The full list of these statements can be found in the supplementary materials.
Note that we cite our use of ChatGPT according to the AI Code of Conduct developed
by Iliada Eleftheriou and Ajmal Mubarik and the University of Manchester
[@iliada_2023]. Two authors rated each statement on topic emotionality and
strength of correlation using Likert scales from 1 to 7. Topic emotionality
had a midpoint at 4, whereas strength of correlation varied between 1 (Not Related At All)
and 7 (Strongly Related). We calculated a quadratic weighted Cohen's Kappa
between the two raters using the **irr** package (version 0.84.1 [@irr]),
in order to penalise larger magnitude disagreements more harshly.
We found agreement above chance for both topic emotionality
($\kappa$ = `r printnum(irr_emot$value, digits = 2)`, *p* `r printp(irr_emot$p.value, add_equals = TRUE)`)
and strength of correlation ($\kappa$ = `r printnum(irr_corr$value, digits = 2)`,
*p* `r printp(irr_corr$p.value, add_equals = TRUE)`), indicating moderate 
levels of agreement in both cases [@cohen_1968; @fleiss_1969].

```{r}
#| label: tbl-pre-test-hi
#| include: true
#| tbl-cap: Pre-test statements that were rated as being strongly correlated.

# read in pre-test csv data

pre_test_statements_hi <- read_csv("data/pre_test_data.csv") %>%
  filter(label == "high_corr") %>%
  select(c("item_no","statement")) %>%
  rename("Statement - Strong Correlation Depicted" = "statement",
         "Item Number" = "item_no")

# make table

kbl(pre_test_statements_hi, booktabs = T)
```

```{r}
#| label: tbl-pre-test-low
#| include: true
#| tbl-cap: Pre-test statements that were rated as being weakly correlated.

# read in pre-test csv data

pre_test_statements_lo <- read_csv("data/pre_test_data.csv") %>%
  filter(label == "low_corr") %>%
  select(c("item_no","statement")) %>% 
  rename("Statement - Weak Correlation Depicted" = "statement",
         "Item Number" = "item_no")

# make table

kbl(pre_test_statements_lo, booktabs = T)
```

Following this, we selected strongly and weakly correlated statements with the
highest level of absolute agreement, resulting in the 14 strongly correlated
statements that can be seen in @tbl-pre-test-hi and the 11 weakly correlated
statements that can be seen in @tbl-pre-test-low. We then tested these 25
statements with a representative UK sample in order
to ascertain consensus on both topic emotionality and strength of correlation.
Doing so allows us to effectively exclude these factors when we analyse the effects
of our atypical scatterplot designs on the propensity for belief change
in our main experiment.

## Method {#sec-method-pre}

### Participants {#sec-participants-pre}

100 participants were recruited using the Prolific.co platform [@prolific]. English fluency and
residency was required for participation, as our main experiment relied on familiarity
with data visualizations from a popular British news source. In addition to 25
experimental items, we included six attention check items, which asked participants
to provide specific answers. No participants failed more than 2 out of 6 attention
check items, and therefore data from all 100 were included in the full analysis
(`r printnum(gender_pre$Male, digits = 1)`% male and `r printnum(gender_pre$Female, digits = 1)`% 
female. Participants' mean age was `r printnum(age_pre$mean, digits = 1)` (*SD* = 
`r printnum(age_pre$sd, digits = 1)`). The average time taken to complete the survey was
`r printnum(time_taken_pre$mean, digits = 1)` minutes (*SD* = `r printnum(time_taken_pre$sd, digits = 1)` minutes).

### Design {#sec-design-pre}

Each participant saw all survey items (@tbl-pre-test-hi and @tbl-pre-test-low),
along with the six attention check items, in a fully randomised order. All
experimental code, materials, and instructions are hosted on GitLab[^4].

[^4]: https://gitlab.pavlovia.org/Strain/beliefs_scatterplots_pretest

### Procedure {#sec-procedure-pre}

The experiment was built using Psychopy [@pierce_2019] and hosted on Pavlovia.org.
Participants were permitted to complete the experiment using a phone, tablet, desktop,
or laptop computer. Participants were first shown the participant information sheet
and were asked to provide consent through key presses in response to consent statements.
They were asked to provide their age in a free text box, followed by their 
gender identity. Participants were told that they would be asked to read statements
about the relatedness between a pair of variables, after which they would have to 
indicate their beliefs about topic emotionality and the strength of correlation suggested
using a pair of sliders. To familiarize themselves with the sliders, they were asked to complete a practice round
in response to the statement "As participation in online experiments increases,
society becomes happier."

## Results {#sec-results-pre}

```{r}
#| label: kappa-pre-test

# Calculate Fleiss' Kappa for 100 raters on emotional valence

pre_test_emot <- beliefs_scatterplots_pretest_tidy %>%
  select(c("participant", "slider_emotion.response", "item_no"))

emot_matrix <- xtabs(slider_emotion.response ~ item_no + participant, data = pre_test_emot)

emot_matrix <- as.matrix(emot_matrix)

emot_kappa <- kappam.fleiss(emot_matrix)

# do the same for strength of belief

pre_test_belief <- beliefs_scatterplots_pretest_tidy %>%
  select(c("participant", "slider_belief.response", "item_no"))

belief_matrix <- xtabs(slider_belief.response ~ item_no + participant, data = pre_test_belief)

belief_matrix <- as.matrix(belief_matrix)

belief_kappa <- kappam.fleiss(belief_matrix)

# calculate mean ratings and standard deviations for each statement

agreement_df_emot <- beliefs_scatterplots_pretest_tidy %>%
  group_by(item_no) %>%
  summarise(mean_emot = mean(slider_emotion.response),
            std_dev_emot = sd(slider_emotion.response)) %>%
  arrange(std_dev_emot)

agreement_df_belief <- beliefs_scatterplots_pretest_tidy %>%
  group_by(item_no) %>%
  summarise(mean_belief = mean(slider_belief.response),
            std_dev_belief = sd(slider_belief.response)) %>%
  arrange(std_dev_belief)

ratings_df <- full_join(agreement_df_belief, agreement_df_emot, by = "item_no")

# calculate average topic emotionality ratings

avg_emot <- ratings_df %>%
  filter(between(mean_emot, 3, 5))

# calculate consensus by summing standard deviations

consensus_df <- avg_emot %>%
  mutate(consensus = std_dev_belief + std_dev_emot) %>% 
  arrange(consensus) %>%
  slice_head(n =  2)
```

All analyses were conducted using R (version `r paste0(R.version$major, ".", R.version$minor)`).
We use the **irr** package to calculate Fleiss' Kappa to measure interrater agreement
on topic emotionality and strength of correlation for the 25 experimental items.
This analysis revealed that participants agreed above chance for both topic emotionality
($\kappa$ = `r printnum(emot_kappa$value, digits = 2)`, *p* `r printp(emot_kappa$p.value, add_equals = TRUE)`)
and strength of correlation ($\kappa$ = `r printnum(belief_kappa$value, digits = 2)`,
*p* `r printp(belief_kappa$p.value, add_equals = TRUE)`).

## Selecting Statements for the Main Experiment

To control for any potential effects of topic emotionality in the main experiment,
we first select statements that represent neutral emotional valence. Statements
with average topic emotionality ratings between 3 and 5 are statements 
`r printnum(add_and_to_numbers(avg_emot$item_no), digits = 0)`. To ascertain
which statements represent the greatest consensus, we add standard deviations in
ratings for topic emotionality and strength of correlation. Due to concerns about experimental
power, and in line with evidence that propensity for belief change is highest
when prior beliefs are not strongly held [@xiong_2022],
we elected at this point to test only the statement corresponding to weak beliefs
about the strength of correlation between the variables in question. We therefore
test statement number `r printnum(add_and_to_numbers(consensus_df$item_no[2]), digits = 0)`,
"Higher consumption of spicy foods is associated with a lower risk of certain types of cancer.",
however we modify the wording so that both variables (food consumption and cancer risk)
are positively correlated, as previous work indicates that the manipulations we use
in the atypical scatterplot condition are able to change estimates of correlation
in positively correlated scatterplots; no work regarding the effects of these
manipulations in negatively correlated scatterplots has been completed.

## Discussion {#sec-discussion-pre}

Fleiss' Kappa values for interrater agreement on both topic emotionality
and strength of correlation scales are low ($\kappa$ = `r printnum(emot_kappa$value, digits = 2)`
and $\kappa$ = `r printnum(belief_kappa$value, digits = 2)` respectively),
however do exceed that which would be expected by chance. We suggest this may be
due to Fleiss' Kappa not being designed with ordinal (Likert scales in this case) data in mind.
In light of this we do not make decisions regarding which statement to use based 
on the values of Fleiss' Kappa observed, but rather on the standard deviations of 
ratings across all raters. Regardless, we do not consider this to be a particular
weakness, as we also test topic emotionality and strength of correlation with participants
in the main study and include these ratings as part of our analysis.

# Main Study: Potential for Belief Change Using Atypical Scatterplots {#sec-main-study}

We test the statement that exhibited the lowest average level of belief about
correlation, and the 2nd highest level of consensus. Modified for directionality,
this statement is therefore: "Higher consumption of plain (non-spicy) foods
is associated with a lower risk of certain types of cancer."

## Introduction

 - hypotheses
 - 

### Defensive Confidence

In line with evidence that those who are more confident in their ability to defend
their own positions are more susceptible to having those positions changed [@albarracin_2004],
we test participants' defensive confidence using a 12-item scale. This scale is 
replicated from previous work in the supplemental material, and has additionally
been utilized more recently [@markant_2023] to explore the potential for attitude
change specifically with regards to correlations in scatterplots. Participants
provide answers to the 12 scale items using a 5 point Likert scale ranging from 1
(*not at all characteristic of me*) to 5 (*extremely characteristic of me*). Analysis
including participants' defensive confidence scores is included in @sec-additional-analyses.

## Stimuli

Recent work has shown that estimates of correlation can be altered when point opacities
and sizes are systematically varied in scatterplots [@strain_2023; @strain_2023b; @strain_2024].
These manipulations have been used in an attempt to correct for a long-standing
underestimation bias observed in correlation perception as it pertains to scatterplots.
As we now aim to test the propensity of these manipulations to affect participants'
beliefs about levels of relatedness, we choose the set of manipulations that
has previously produced the most dramatic effect on correlation estimates; namely,
the combination of typical orientation size and opacity manipulations provided
by Strain et al [@strain_2024]. Here, the size and opacity of a certain
scatterplot point is lowered as a function of that point's residual error using
equation 1:

\begin{equation}
  point_{size/opacity} = 1 - b^{residual}
\end{equation}

In order to facilitate comparison to this work, we use the same protocol to produce
scatterplots for our atypical condition. This includes creating scatterplots
with 128 points, using *b* = 0.25, employing a scaling factor and constant for point size, and using an opacity
floor to ensure point visibility, as this has been an issue in previous work. As there
is evidence that people are less likely to update strongly held beliefs following
viewing scatterplot visualizations [@markant_2023 look for more], we selected a
correlative statement that was judged as representing weakly correlated variables.
Thus, in order to induce belief change, participants only viewed scatterplots
representing **strongly** correlated variables (0.6 > *r* > 0.99). We used **ggplot**
in R to create plots that echoed the style of a British news broadcaster and fictitiously
claimed the data to be provided by the British National Health service. Examples
of typical and atypical data-identical scatterplots can be seen in @fig-main-examples.

```{r}
#| label: fig-example-plots
#| include: true
#| out-width: "100%"
#| fig-cap: Examples of the experimental stimuli used with an \textit{r} value of 0.6.

# function for making slopes df

slope_function <- function(my_desired_r) {
  set.seed(1234)
  
  my_sample_size = 128
  
  mean_variable_1 = 5
  sd_variable_1 = 1
  
  mean_variable_2 = 76
  sd_variable_2 = 5
  
  mu <- c(mean_variable_1, mean_variable_2) 
  
  myr <- my_desired_r * sqrt(sd_variable_1) * sqrt(sd_variable_2)
  
  mysigma <- matrix(c(sd_variable_1, myr, myr, sd_variable_2), 2, 2) 
  
  corr_data = as_tibble(mvrnorm(my_sample_size, mu, mysigma, empirical = TRUE))
  
  corr_model <- lm(V2 ~ V1, data = corr_data)
  
  my_residuals <- abs(residuals(corr_model))
  
  data_with_resid <- round(cbind(corr_data, my_residuals), 2)
  
  slopes <- data_with_resid %>%
    mutate(slope_0.25 = 1-(0.25)^my_residuals) %>%
    mutate(slope_inverted = (1 + (0.25)^ my_residuals)-1) %>%
    mutate(slope_inverted_floored = pmax(0.2,(1+(0.25)^my_residuals)-1)) %>%
    mutate(typical = 0.033) %>%
    mutate(standard_alpha = 1)
  
  return(slopes)
}

# manually specify variables from slopes df

slopes <- slope_function(0.6)
slopeI <- (slopes$slope_inverted)
slopeI_floored <- (slopes$slope_inverted_floored)
typical <- (slopes$typical)
standard_alpha <- (slopes$standard_alpha)

# function for creating example plots

example_plot_function <- function(slopes, my_desired_r, size_value, opacity_value, theme) {
  
  p <- ggplot(slopes, aes(x = V1, y = V2)) +
    scale_size_identity() +
    scale_alpha_identity() +
    geom_point(aes(size =  6*(size_value + 0.3), alpha = opacity_value), shape = 16) +  
    geom_hline(yintercept = 68, size = 1, colour="#333333") +
    geom_segment(x = 0, xend = 10, y = 66.2, yend = 66.2, size = 0.3, colour="#585858") +
    bbc_style() +
    theme(axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          title = element_text(size = 14),
          plot.subtitle = element_text(size = 9),
          plot.caption = element_text(size = 9, hjust = -0.02),
          plot.margin = unit(c(0,0,1,0), "cm")) +
    labs(title = "Spicy Foods",
         subtitle = "Higher consumption of plain (non-spicy) foods\nis associated with a higher risk of certain types of cancer.",
         caption = "Source: NHS England") +
    annotate("text", x = 3, y = 67, label = "Less plain Diet") +
    annotate("text", x = 7, y = 67, label = "More plain Diet") +
    annotate("text", x = 1, y = 71, label = "Fewer Cancer\nDiagnoses", angle = 90) +
    annotate("text", x = 1, y = 80, label = "More Cancer\nDiagnoses", angle = 90) +
    coord_cartesian(clip = "off")
  
  return(p)
  
}

# use ggarrange to arrange plots, include annotation geoms for plot titles

example_plots <- ggarrange(example_plot_function(slopes, 0.6, typical, standard_alpha, bbc_style()),
                        example_plot_function(slopes, 0.6, slopeI, slopeI_floored, bbc_style()),nrow = 1) +
  annotate(geom = "text",
           label = "Typical Scatterplot",
           x = 0.25,
           y = 0.04,
           size = 7) +
  annotate(geom = "text",
           label = "Atypical Scatterplot",
           x = 0.75,
           y = 0.04,
           size = 7) +
    annotate(geom = "segment",
           x = 0.077,
           xend = 0,
           y = 0.04,
           yend = 0.04,
           size = 0.6,
           colour = "grey",
           arrow = arrow(length = unit(0.15, "cm"), angle = 90)) +
  annotate(geom = "segment",
           x = 0.935,
           xend = 1,
           y = 0.04,
           yend = 0.04,
           size = 0.6,
           colour = "grey",
           arrow = arrow(length = unit(0.15, "cm"), angle = 90)) +
      annotate(geom = "segment",
           x = 0.43,
           xend = 0.5,
           y = 0.04,
           yend = 0.04,
           size = 0.6,
           colour = "grey",
           arrow = arrow(length = unit(0.15, "cm"), angle = 90)) +
  annotate(geom = "segment",
           x = 0.565,
           xend = 0.5,
           y = 0.04,
           yend = 0.04,
           size = 0.6,
           colour = "grey",
           arrow = arrow(length = unit(0.15, "cm"), angle = 90))

rm(slopeI, slopeI_floored, typical, standard_alpha, slopes) # clean up

example_plots
```

## Method

### Participants

Participants were recruited using Prolific.co [@prolific]. English fluency and UK
residency was required for participation, as well as normal or corrected-to-normal
vision, and having not participated in any of our previous studies regarding 
correlation perception in scatterplots [@strain], as these represented earlier testing 
of the alternative designs we employ in the atypical scatterplot condition. Data were
collected from 77 participants for each condition. 2 participants failed more than
2 out of 4 attention check questions for each condition, meaning their data were
excluded per pre-registration stipulations. Data from the remaining 150 participants
were included in the full analysis (`r printnum(gender_main$Male, digits = 1)`% male,
`r printnum(gender_main$Female, digits = 1)`% female, and `r printnum(gender_main$'Non-binary', digits = 1)`% non-binary).
Participants' mean age was `r printnum(age_main$mean, digits = 1)`
(*SD* = `r printnum(age_main$sd, digits = 1)`). Participants' mean graph literacy
score was `r printnum(literacy$mean, digits = 1)` (*SD* = `r printnum(literacy$sd, digits = 1)`)
out of 30, their mean defensive confidence score was `r printnum(def_con$mean, digits = 1)`
(*SD* = `r printnum(def_con$sd, digits = 1)`) out of 60, and their mean rating of 
topic emotionality was `r printnum(topic_emo$mean, digits = 1)`
(*SD* = `r printnum(topic_emo$sd, digits = 1)`) on a 7 point Likert scale.
On average, participants took `r printnum(time_taken_main$mean, digits = 1)`
minutes to complete the experiment (*SD* = `r printnum(time_taken_main$sd)`).

### Design

We employed a between-participants design. Each participants was randomly assigned
to either group A, in which case they viewed typical scatterplots, or group B,
in which they viewed atypical scatterplots designed deliberately to elicit higher
levels of belief change. Participant saw all experimental items for their group, along
with 4 attention check items, in a fully randomised order. All experimental code, 
materials, and instructions are hosted on GitLab as two separate experiments [^1] [^2]

[^1]: removed for anon
[^2]: removed for anon

### Procedure

We use PsychoPy [@pierce_2019] to build our experiment and Pavlovia.org
to host it. Participants were permitted to complete the experiment on a desktop
or laptop computer. We elected to prevent participants from using a phone or
tablet to complete the experiment in line with evidence that differences in on-screen sizes
of data visualizations can alter perceptions [cleveland_1982]. Participants were
first shown the participant information sheet and asked to provide consent
through key presses in response to consent statements. They were, again,
asked to provide their age and gender identity. Participants then completed the 12-item
Defensive Confidence scale described by Albarracín and Mitchell [@albarracin_2004]
and the 5-item Subjective Graph Literacy scale [@garcia_2016][^5].
Following instructions, which included descriptions of scatterplots and Pearson's *r*.
In order to give legitimacy to our data visualizations with the hope of maximizing
any potential belief change, participants were told that the graphs were taken
from a well-known British news source, but that the identity of this source had 
been obscured. In order to promote participant engagement with the visualizations,
participants were instructed to use a slider to estimate the correlation displayed
in each scatterplot; no hypotheses were made based on these data, however they
are discussed in light of recent literature in @sec-correlation-ratings.
Participants then had a chance to practice using the slider, before being 
asked their belief about topic emotionality and the relatedness between variables
described in our chosen statement. Following the completion of the experimental
trials, participants were tested again on their beliefs about relatedness, and 
then debriefed that the data they saw were fictional. Interspersed among
the experimental items were 4 attention check trials which explicitly
asked participants to set the slider to 0 or 1.

[^5]: The inclusion of this scale was not specified in the pre-registration.

## Results

```{r}
#| label: model-rating-absolute
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# first re-wrangle the df into long format with regards to ratings

diff_abso_df_long <- main_exp_tidy %>%
  pivot_longer(cols = c("pre_bel", "post_bel"),
               values_to = "rating",
               names_to = "rating_time")

# create model

diff_abso_model <- buildmer(rating ~ rating_time +
                         (1  | participant) +
                         (1 + condition | item_no),
                       data = diff_abso_df_long)
```

```{r}
#| label: assign-slot-absolute

# assign to slot for further use

diff_abso_model <- diff_abso_model@model
```

```{r}
#| label: model-cmpr-diff-abso
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# build comparison model with fixed effects term removed

diff_abso_model_cmpr <- comparison(diff_abso_model)
```

```{r}
#| label: anova-diff-abso

# run ANOVA between experimental and comparison models, ouputs stats in global env

anova_results_mixed(diff_abso_model, diff_abso_model_cmpr)
```

All analyses were conducted using R (version `r paste0(R.version$major, ".", R.version$minor)`).
The **buildmer** (version 2.11 [@buildmer]) and **lme4** (version 1.1-35.3 [@lme4])
packages were used to build linear mixed effects models. Semi-partial R^2^,
which is presented in lieu of traditional measures of effect size,
was calculated using the **r2glmm** package (version 0.1.2 [@r2glmm]). To test the first hypothesis,
that ratings of strength of relatedness would be different before and after participants
viewed experimental items, we build a model whereby the rating is predicted by the point
in time the participant made it. Our first hypothesis was supported; there was a 
significant difference in ratings of strength of relatedness made before and after
participants viewed the experimental plots.

```{r}
#| label: tbl-abso-diff
#| include: true
#| tbl-cap: Statistics for the significant main effect of rating time. Semi-partial R^2^ is also incuded.

make_sig_table_abso(diff_abso_model)
```

```{r}
#| label: fig-abso-descriptives
#| include: true
#| out-width: "100%"
#| fig-asp: 0.5
#| fig-cap: Boxplots showing ranges, interquartile ranges, medians (vertical lines) and means for participants' ratings of strength of relatedness before and after viewing either typical or atypical scatterplots.

diff_abso_df_long %>%
  mutate("condition" = recode(condition,
                            "A" = "Atypical\nScatterplot",
                            "T" = "Typical\nScatterplot"),
         "rating_time" = recode(rating_time,
                                "pre_bel" = "Pre",
                                "post_bel" = "Post")) %>%
  ggplot(aes(x = condition, y = rating, fill = rating_time)) +
  geom_boxplot(outliers = F, key_glyph = "polygon") +
  scale_color_brewer(palette = "Dark2", aesthetics = "fill") +
  stat_summary(fun.y = mean, geom = "point", shape = 15, size = 4, position = position_dodge(0.75)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 7)) +
  labs(x = "",
       y = "Rating",
       fill = "Rating Time") +
  theme_ggdist() +
  theme(legend.position = c(0.13, 0.12),
        legend.direction = "horizontal",
        legend.box = "horizontal",
        legend.key.size = unit(1.15, "line"),
        legend.title.position = "top",
        axis.line.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_text(size = 11),
        axis.text.x = element_text(size = 11)) +
  guides(shape = F) +
  guides(fill = guide_legend(override.aes = list(shape = NA), reverse = T)) + 
  coord_flip() 
```

A likelihood ratio test revealed that the model
including time of rating as a predictor explained significantly more variance than
the null ($\chi^2$(`r in_paren(diff_abso_model.df)`) = `r printnum(diff_abso_model.Chisq)`,
*p* `r printp(diff_abso_model.p, add_equals = TRUE)`). This model had random
intercepts for participants. Statistical testing providing
support for this hypothesis is shown in @tbl-abso-diff. @fig-abso-descriptives
shows means and boxplots for ratings of strength of relatedness before and after
viewing scatterplots in either the typical or atypical condition.

```{r}
#| label: model-relative
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# modelling effect of condition on difference between belief strength before and
# after viewing plots

relative_model <- buildmer(belief_diff ~ condition +
                              (1 | participant) +
                              (1 + condition | item_no),
                            data = main_exp_tidy)
```

```{r}
#| label: model-relative-assign-slot
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# assign relative model to slot for further use

relative_model <- relative_model@model
```

```{r}
#| label: model-relative-cmpr
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# build null model for comparison. Model has no random effects terms, so do it manually

relative_model_cmpr <- lm(belief_diff ~ 1, data = main_exp_tidy)
```

```{r}
#| label: model-relative-results
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# extract ANOVA results for relative vs null model comparison

anova_results_linear(relative_model_cmpr, relative_model)
```

Our second hypothesis, that the difference between ratings of strength of relatedness
before and after viewing experimental plots would be greater when participants
were assigned to the atypical scatterplot condition, also received support. We built
a linear mixed effects model whereby the difference was predicted by the condition
the participant was assigned to. A likelihood ratio test revealed that the model
including condition as a predictor explained significantly more variance than the null
($F$(`r in_paren(relative_model.df)`) = `r printnum(relative_model.F)`,
*p* `r printp(relative_model.p, add_equals = TRUE)`). This model contains no
random effects terms. Test statistics, along with semi-partial R^2^ can be seen in @tbl-rel.

```{r}
#| label: tbl-rel
#| include: true
#| tbl-cap: Statistics for the significant main effect of condition on the difference between pre and post scatterplot viewing ratings for typical and atypical plots. Semi-partial R^2^ is also incuded.

make_sig_table_rel(relative_model)
```

### Additional Analyses

```{r}
#| label: add-analyses
#| eval: !expr params$eval_models
#| cache: !expr params$eval_models

# I don't know a way to do this programmatically, as we're doing interactive effects
# so for now just do it manually

lit_model <- lm(belief_diff ~ 1 + condition * literacy ,
          data = main_exp_tidy)

anova_results_linear(relative_model, lit_model)

dc_model <- lm(belief_diff ~ 1 + condition * dc_score,
          data = main_exp_tidy)

anova_results_linear(relative_model, dc_model)

emo_model <- lm(belief_diff ~ 1 + condition * slider_emotion.response,
                data = main_exp_tidy)

anova_results_linear(relative_model, emo_model)
```


```{r}
#| label: tbl-lit-stat
#| include: true
#| tbl-cap: Statistics for main effects and interactions when scores on the Subjective Graph Literacy test are included in the model.

make_sig_table_lit(lit_model)
```

```{r}
#| label: tbl-dc-stat
#| include: true
#| tbl-cap: Statistics for main effects and interactions when scores on the Defensive Confidence test are included in the model.

make_sig_table_dc(dc_model)
```

```{r}
#| label: tbl-emo-stat
#| include: true
#| tbl-cap: Statistics for main effects and interactions when likert ratings of topic emotionality are included in the model.

make_sig_table_emo(emo_model)
```

We find effects of participants' scores on a Defensive Confidence test 
($F$(`r in_paren(dc_model.df)`) = `r printnum(dc_model.F)`,
*p* `r printp(dc_model.p, add_equals = TRUE)`), participants' scores on a graph
literacy test [@garcia_2016] ($F$(`r in_paren(lit_model.df)`) = `r printnum(lit_model.F)`,
*p* `r printp(lit_model.p, add_equals = TRUE)`), and of how emotional participants'
rated the chosen correlative statement before beginning the block of trials
($F$(`r in_paren(emo_model.df)`) = `r printnum(emo_model.F)`,
*p* `r printp(emo_model.p, add_equals = TRUE)`). There are significant fixed
and interactive effects present for both graph literacy and defensive
confidence. @tbl-lit-stat and @tbl-dc-stat contain statistics, including partial
R^2^ for both factors respectively. We found no significant fixed effect of 
emotionality ratings, but an interaction was observed, which can be see in 
@tbl-emo-stat. Given the low estimates and effects sizes
associated with the fixed effects of defensive confidence and graph literacy,
and the the absence of a fixed effect of emotionality, we instead choose
to focus on the interactions themselves; see @sec-add-analyses for further discussion
of these effects.

## Discussion

### Graph Literacy, Defensive Confidence, and Topic Emotionality {#sec-add-analyses}

```{r}
#| label: fig-lit-smooth
#| include: true
#| fig-cap: Illustrating how differences in beliefs about strength of relatedness change as a function of partcipants' scores on the subjective graph literacy test; smoothed curves are shown for the grand mean, as well as separately for atypical and typical scatterplot presentation conditions.

main_exp_tidy %>%
  mutate("condition" = recode(condition,
                              "A" = "Atypical",
                              "T" = "Typical")) %>%
  ggplot(aes(x = literacy, y = belief_diff)) +
    geom_textpath(stat = "smooth",
            method = "loess",
            span = 1,
            vjust = -0.3,
            hjust = 1,
            size = 5,
            aes(colour = condition, label = condition)) +
    geom_textpath(stat = "smooth",
            method = "loess",
            span = 1,
            vjust = -0.3,
            hjust = 1,
            size = 5,
            aes(x = literacy, y = belief_diff, label = "Grand Mean")) +
  scale_color_brewer(palette = "Set2", aesthetics = "color") +
  theme_ggdist() +
  theme(legend.position = "none") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 15)) +
  labs(x = "Subjective Graph Literacy Score (/30)",
       y = "Difference in Beliefs about Strength of Relatedness") +
  annotate(geom = "segment",
           x = 9,
           y = 1.25,
           xend = 9,
           yend = 0.75,
           size = 0.8,
           colour = "black",
           arrow = arrow(length = unit(.25, "cm"), angle = 45)) +
  annotate(geom = "text",
           x = 13,
           y = 1,
           label = "Lower numbers represent\nsmaller changes in belief",
           colour = "black")
```

Mean differences in pre and post plot-viewing ratings of strength of relatedness
can be seen in @fig-lit-smooth. Generally, participants who scored higher on
a graph literacy test experienced smaller changes in their ratings of strength
of relatedness. While the effect size associated with this interaction is small
(~0.01, see @tbl-lit-stat), and does not alter the overall pattern of results, 
it is in line with previous work suggesting that those with higher levels of
graph or visualization literacy show better performance in inference tasks
related to visualizations [@canham_2010], are more capable of describing effects
that visualisations aim to communicate [@shah_2011], and are able to preferentially
attend to relevant features of visualizations to a greater degree [@okan_2015], than those with
lower levels of graph literacy. In the present study, we provide evidence
that those with greater levels of graph literacy are *less susceptible* to having
their beliefs changed by visualizations, although this difference is somewhat
negated if participants viewed atypical scatterplots, in which case levels
of belief change were relatively consistent. We hypothesize that this is due
to the non-standard nature of the atypical scatterplots used in that condition.

```{r}
#| label: fig-dc-smooth
#| include: true
#| fig-cap: Illustrating how differences in beliefs about strength of relatedness change as a function of partcipants' scores on the defensive confidence test; smoothed curves are shown for the grand mean, as well as separately for atypical and typical scatterplot presentation conditions.

main_exp_tidy %>%
  mutate("condition" = recode(condition,
                              "A" = "Atypical",
                              "T" = "Typical")) %>%
  #filter(dc_score < 53) %>%
  ggplot(aes(x = dc_score, y = belief_diff)) +
    geom_textpath(stat = "smooth",
            method = "loess",
            span = 1,
            vjust = -0.3,
            hjust = 1,
            size = 5,
            aes(colour = condition, label = condition)) +
    geom_textpath(stat = "smooth",
            method = "loess",
            span = 1,
            vjust = 1.3,
            hjust = 0.8,
            size = 5,
            aes(x = dc_score, y = belief_diff, label = "Grand Mean")) +
  scale_color_brewer(palette = "Set2", aesthetics = "color") +
  theme_ggdist() +
  theme(legend.position = "none") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 15)) +
  labs(x = "Defensive Confidence Score (/60)",
       y = "Difference in Beliefs about Strength of Relatedness") +
  annotate(geom = "segment",
           x = 25,
           y = 1.25,
           xend = 25,
           yend = 0.75,
           size = 0.8,
           colour = "black",
           arrow = arrow(length = unit(.25, "cm"), angle = 45)) +
  annotate(geom = "text",
           x = 33,
           y = 1,
           label = "Lower numbers represent\nsmaller changes in belief",
           colour = "black")
```

```{r}
#| label: dc-max

dc_max_typical <- main_exp_tidy %>% filter(condition == "T") %>%
  summarise(max = max(dc_score))
```

We observe an opposing pattern of results when examining the effects of defensive confidence
on participants' propensity for belief change. Generally, participants who scored 
more highly on the defensive confidence test experienced greater levels of belief
change. This is in line with evidence that those who are more confidence in their
ability to defend their own beliefs are more liable to having those beliefs changed
in light of evidence [@albarracin_2004]. An extended analysis of the defensive
confidence data collected by Markant et al., [@markant_2023] revealed a similar
pattern of results; participants who scored more highly on a defensive confidence
test experienced greater levels of belief change after viewing scatterplot visualizations[^1],
although the effect in that case is much less pronounced due to differences
in study design. This effect is explained as being due to those with a greater degree
of confidence in their own ability to defend their ideas engage with information
with lower levels of attention to the fact it opposes their beliefs. The present
study provides evidence in favour of this phenomenon.

While the general pattern of results is expected based on previous work, the interaction
present between defensive confidence and scatterplot condition is novel (see @fig-dc-smooth).
It would appear that despite following the normal pattern of results for low to 
moderate levels of defensive confidence, those participants who viewed the typical
scatterplots experienced a drop in belief change as defensive confidence increased
past ~ 36/60. There are several potential explanations in the data; firstly, that the 75 participants
who formed the typical scatterplot presentation group had a more restricted range
of defensive confidence scores, topping out at `r printnum(dc_max_typical$max, digits = 0)`;
and secondly, that the unfamiliar nature of the atypical scatterplots was protective
against against an unexpected, standard behaviour whereby very high levels of 
defensive confidence decrease susceptibility to belief change. 

[^1]: The GitHub repository associated with this paper contains an R script that performs this analysis.


```{r}
#| label: fig-emo-smooth
#| include: true
#| fig-cap: Illustrating how differences in beliefs about strength of relatedness change as a function of partcipants' scores on the subjective graph literacy test; smoothed curves are shown for the grand mean, as well as separately for atypical and typical scatterplot presentation conditions.

main_exp_tidy %>%
  mutate("condition" = recode(condition,
                              "A" = "Atypical",
                              "T" = "Typical")) %>%
  ggplot(aes(x = slider_emotion.response, y = belief_diff)) +
    geom_textpath(stat = "smooth",
            method = "loess",
            span = 1,
            vjust = 1.4,
            hjust = .95,
            size = 5,
            aes(colour = condition, label = condition)) +
    geom_textpath(stat = "smooth",
            method = "loess",
            span = 1,
            vjust = -0.3,
            hjust = 1,
            size = 5,
            aes(x = slider_emotion.response, y = belief_diff, label = "Grand Mean")) +
  scale_color_brewer(palette = "Set2", aesthetics = "color") +
  theme_ggdist() +
  theme(legend.position = "none") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 7)) +
  labs(x = "Topic Emotionality Rating (/7)",
       y = "Difference in Beliefs about Strength of Relatedness") +
  annotate(geom = "segment",
           x = 1.5,
           y = 1.25,
           xend = 1.5,
           yend = 0.75,
           size = 0.8,
           colour = "black",
           arrow = arrow(length = unit(.25, "cm"), angle = 45)) +
  annotate(geom = "text",
           x = 2.75,
           y = 1,
           label = "Lower numbers represent\nsmaller changes in belief",
           colour = "black")
```

Examining @fig-emo-smooth, it would appear that the interaction we see between 
belief change and topic emotionality is driven by those participants who
rated the statement low on emotionality having different levels of belief change
between atypical and typical scatterplot presentation conditions; in this case levels 
of belief change were much higher for the group that viewed the atypical 
scatterplot designs. There is a broad research space regarding emotionality and
data visualization [@lan_2023], and it is clear from previous work that emotion
affects perception, cognition, and behaviour [@phelps_2006; @harrison_2013, probably look for more]
with regard to data visualization. Harrison et al. [@harrison _2013] found that 
participants who were positively primed performed better on a low-level
visual judgement task. Comparison of this work to the current is difficult, as
*success* on our task is hard to define. From our data, it is unclear why participants
who rated the correlative statement as "strongly negative" experienced significantly
higher levels of belief change when they viewed atypical scatterplots. We decided to
capture the emotionality of the statement simply because we had done so in
the pre-study; we made no predictions based on it, and chose a broad measure of
emotionality, meaning we did not capture intensity separately from valence. It 
may be that the strong negative emotion amplified the effect of scatterplot condition
in a way that strong positive emotion did not, although it is unclear why this has occurred.
Further experimental work is required to provide concrete explanations for the interactive
effects of graph literacy, defensive confidence, and statement emotionality
in the current experimental paradigm; as the investigation of these is not the prime
goal of the present study, however, we do not consider this a shortcoming.

# General Discussion

# Limitations

# Future Work

- maybe something about attitudes
- understanding how DC and literacy interact
- extending beliefs and attitudes to behavioural change (golden ticket really)


# Conclusion

# References {.unnumbered}
